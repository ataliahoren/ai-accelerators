{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced-notebooks template (aka quick hits)\n",
    "Author: DS McStatsy \\\n",
    "Version Date: [date]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission Process:\n",
    "1. Create a branch in accelerators_dev/advanced_experimentation. Build out your notebook\n",
    "2. Ensure the checklist below is satisified before submitting\n",
    "3. Create a pull request to the accelerators_dev/advanced_experimentation folder\n",
    "4. tag @cmccann11, @joao, and @y2ee201 for review\n",
    "5. The first review is code review. Afer that, is copy review by the docs team\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checklist for an accelerator  - style\n",
    "1. Ensure authors and date are the top of the notebook (or in the readme), as well as the DataRobot logo and integration partner logo (eg AWS) if applicable\n",
    "2. Ensure you have/address: \n",
    "    - A clear title, identify major integrations if present (e.g. Huggingface, Kubeflow, AWS)\n",
    "    - What is the problem? \n",
    "    - Why is it a problem for data scientists/ our users? \n",
    "    - How we can solve it? \n",
    "    - A few examples to support your statement, covering WHEN and WHY you would use this approach \n",
    "    - What's the benefit of doing this\n",
    "    - A list of what they will learn (steps to achieve HOW)\n",
    "    - ** Each major section of code maps to an item on the list of what they will learn. \n",
    "3. For each header in the body, use the same wording as the numbered section at the top\n",
    "4. Ensure there is markdown explaining WHY each section of code is coming up\n",
    "5. Ensure comments in the code block indicate WHAT is happening (assume people don't know our API very well).Assume your reader can understand python, but is at a beginner level when it comes to our Python SDK  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist for code\n",
    "1. If importing data, use a standard method in this folder or the end-to-end notebooks \n",
    "2. use project.analyze_and_model \n",
    "3. use dr.Client() to auth in with api and token, we will use this for tracking in Kibana\n",
    "4. indicate your version of the datarobot package, and use a requirements.txt if using multiple packages \n",
    "5. The notebook should run without error\n",
    "6. Minimize hardcoded variables; you want this to run, but also want them to easily adapt it to their own data\n",
    "7. Be aware of how other notebooks are doing certain functions. We are early and will standardize, but we don't want 15 different ways of getting the top model from the leaderboard. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some good examples  \n",
    "- https://github.com/datarobot-community/ai-accelerators/blob/main/end-to-end/Databricks_End_To_End.ipynb\n",
    "- https://github.com/datarobot-community/ai-accelerators/blob/main/end-to-end/GCP%20DataRobot%20End%20To%20End.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ************** Notebook below ********************** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear title\n",
    "what the notebook covers  \n",
    "when you would use it (or not)  \n",
    "why it  matters  \n",
    "how - To tackle X, we will do Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you will learn\n",
    "1. XYZ (eg load data)\n",
    "2. XYZ (eg compute custom thing)\n",
    "3. XYZ (eg evaluate different targets...)\n",
    "N. XYZ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. XYZ (eg load data) - example below from Brent\n",
    "### Setup\n",
    "\n",
    "#### Import libraries\n",
    "\n",
    "The first cell of the notebook imports necessary packages, and sets up the connection to the DataRobot platform. There are also optional values that can be provided to use an existing project and deployment - if they are omitted then a new Autopilot session will be kicked off and a new deployment will be created using DataRobot's recommended model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datarobot as dr\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from py4j.java_gateway import java_import\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "import requests\n",
    "import time\n",
    "\n",
    "api_key = \"\" # Get this from the Developer Tools page in the DataRobot UI\n",
    "endpoint = \"https://app.datarobot.com/\" # This should be the URL you use to access the DataRobot UI\n",
    "\n",
    "dr.Client(endpoint=\"%sapi/v2\" % (endpoint), token=api_key)\n",
    "\n",
    "# Set these to empty strings to create a new project and/or deployment\n",
    "project_id = \"\"\n",
    "deployment_id = \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to DataRobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.Client()\n",
    "# The `config_path` should only be specified if the config file is not in the default location described in the API Quickstart guide\n",
    "# dr.Client(config_path = 'path-to-drconfig.yaml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "Here you'll pull in some data to work with. If a data table is available, you can provide the input table name, destination table name, and target feature in this cell. If none of those are provided, load the sample dataset provided by Databricks. This is also where any necessary data preparation would occur before sending the dataset to DataRobot. Note that DataRobot does not currently ingest Spark dataframes directly, so the dataframe will need to be converted to a Pandas dataframe prior to upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_table = \"\"\n",
    "scoring_table = \"\"\n",
    "target = \"\"\n",
    "\n",
    "if training_table == \"\":\n",
    "    scoring_table = \"white_wine_scored\"\n",
    "    target = \"quality\"\n",
    "    input_df = spark.read.option(\"header\",True).option(\"delimiter\",\";\").csv(\"dbfs:/databricks-datasets/wine-quality/winequality-white.csv\")\n",
    "    input_df = input_df.select([col(column).alias(column.replace(\" \",\"_\")) for column in input_df.columns])\n",
    "else:\n",
    "    input_df = sql(\"select * from %s\" % (training_table))\n",
    "\n",
    "df = input_df.toPandas()\n",
    "display(input_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XYZ (eg compute custom thing)\n",
    "\n",
    "In order to explore multiple target framings, we need to apply a transform on....etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic =df.apply(lambda x: x.super_magic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}