{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b4473b3-339a-493e-8fa5-b04800b668c5",
   "metadata": {},
   "source": [
    "# End-to-end demand forecasting and retraining workflow\n",
    "\n",
    "Author: Andrii Kruchko\n",
    "\n",
    "Version Date: 03/27/2023\n",
    "\n",
    "[Reference DataRobot's API documentation](https://docs.datarobot.com/en/docs/api/reference/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56477c3-0743-4330-885a-d798e7510ad7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook illustrates an end-to-end demand forecasting workflow in DataRobot.  Time series forecasting in DataRobot has a huge suite of tools and approaches to handle highly complex multiseries problems. These include:\n",
    "\n",
    "- Automatic feature engineering and the creation of lagged variables across multiple data types, as well as training dataset creation.\n",
    "- Diverse approaches for time series modeling with text data, learning from cross-series interactions, and scaling to hundreds or thousands of series.\n",
    "- Feature generation from an uploaded calendar of events file specific to your business or use case.\n",
    "- Automatic backtesting controls for regular and irregular time series.\n",
    "- Training dataset creation for an irregular series via custom aggregations.\n",
    "- Segmented modeling, hierarchical clustering for multi-series  models, text support, and ensembling.\n",
    "- Periodicity and stationarity detection and automatic feature list creation with various differencing strategies.\n",
    "- Cold start modeling on series with limited or no history.\n",
    "- Insights for models.\n",
    "- Data and accuracy drift monitoring.\n",
    "- Automated retraining.\n",
    "\n",
    "This notebook demonstrates retraining policies with DataRobot MLOps deployments. \n",
    "\n",
    "The dataset consists of 50 series (46 SKUs across 22 stores) over a two year period with varying series history, typical of a business releasing and removing products over time.\n",
    "\n",
    "DataRobot will be used for the model training, selection, deployment, and making predictions.  Snowflake will work as a data source for both training and testing, and as a storage to write predictions back. This workflow, however, applies to any data source, e.g. Redshift, S3, Big Query, Synapse, etc. For examples of data loading from other environments, check out the other end-to-end examples in this GitHub repo. \n",
    "\n",
    "The notebook covers the following steps:\n",
    "\n",
    "- [Ingest the data from Snowflake into AI Catalog within DataRobot](#data_prep)\n",
    "- [Run a new DataRobot project](#modeling)\n",
    "- [Deploy the recommended model](#deployment)\n",
    "- [Set up automated retraining](#retr)\n",
    "- [Define and run a job to make predictions and write them back into Snowflake](#preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39eadba-f1b5-4f0f-87e3-2541600c20e6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Optional: Import public demo data\n",
    "\n",
    "For this workflow, you can download publicly available datasets (training, scoring, and calendar data) from DataRobot's S3 bucket to your database or load them into your DataRobot instance. \n",
    "\n",
    "If you are using Snowflake, you will need to update the fields below with your Snowflake information. The data is loaded and created in your Snowflake instance. You will also need the following files found in the same repo as this notebook:\n",
    "\n",
    "* dr_utils.py\n",
    "* datasets.yaml\n",
    "\n",
    "Once you are done with this notebook, remember to delete the data from your Snowflake instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c81648-cd2c-4c50-aefc-c01fad05f8bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#requires Python 3.8 or higher\n",
    "from dr_utils import prepare_demo_tables_in_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8621c21-fcb3-45c1-bcfd-771943e66c1e",
   "metadata": {},
   "source": [
    "Fill out the credentials for your Snowflake instance. You will need write access to a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd58b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_user = 'your_username' # Username to access Snowflake database\n",
    "db_password = 'your_password' # Password \n",
    "account = 'account' # Snowflake account identifier\n",
    "db = 'YOUR_DB_NAME' # Database to Write_To\n",
    "warehouse = 'YOUR_WAREHOUSE' # Warehouse \n",
    "schema = 'YOUR_SCHEMA' # Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b9687-4a39-4161-bf91-a26794bc237f",
   "metadata": {},
   "source": [
    "Use the util function to pull the data from DataRobot's public S3 and import into your Snowflake instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55f1e049-f95b-4ba2-9d07-0a811c19b33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "table: ts_demand_forecasting_train\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORE_SKU</th>\n",
       "      <th>DATE</th>\n",
       "      <th>UNITS</th>\n",
       "      <th>UNITS_MIN</th>\n",
       "      <th>UNITS_MAX</th>\n",
       "      <th>UNITS_MEAN</th>\n",
       "      <th>UNITS_STD</th>\n",
       "      <th>TRANSACTIONS_SUM</th>\n",
       "      <th>PROMO_MAX</th>\n",
       "      <th>PRICE_MEAN</th>\n",
       "      <th>STORE</th>\n",
       "      <th>SKU</th>\n",
       "      <th>SKU_CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>store_130_SKU_120931082</td>\n",
       "      <td>2019-05-06</td>\n",
       "      <td>388.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>55.428571</td>\n",
       "      <td>8.182443</td>\n",
       "      <td>243.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.8</td>\n",
       "      <td>store_130</td>\n",
       "      <td>SKU_120931082</td>\n",
       "      <td>cat_1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>store_130_SKU_120931082</td>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>318.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>45.428571</td>\n",
       "      <td>8.079958</td>\n",
       "      <td>210.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.8</td>\n",
       "      <td>store_130</td>\n",
       "      <td>SKU_120931082</td>\n",
       "      <td>cat_1160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 STORE_SKU        DATE  UNITS  UNITS_MIN  UNITS_MAX  \\\n",
       "0  store_130_SKU_120931082  2019-05-06  388.0       44.0       69.0   \n",
       "1  store_130_SKU_120931082  2019-05-13  318.0       37.0       62.0   \n",
       "\n",
       "   UNITS_MEAN  UNITS_STD  TRANSACTIONS_SUM  PROMO_MAX  PRICE_MEAN      STORE  \\\n",
       "0   55.428571   8.182443             243.0        1.0        44.8  store_130   \n",
       "1   45.428571   8.079958             210.0        1.0        44.8  store_130   \n",
       "\n",
       "             SKU SKU_CATEGORY  \n",
       "0  SKU_120931082     cat_1160  \n",
       "1  SKU_120931082     cat_1160  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing ts_demand_forecasting_train to snowflake from:  https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/ts_demand_forecasting_train.csv\n",
      "******************************\n",
      "table: ts_demand_forecasting_calendar\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>event_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>New Year's Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>New Year's Day (Observed)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                 event_type\n",
       "0  2017-01-01             New Year's Day\n",
       "1  2017-01-02  New Year's Day (Observed)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing ts_demand_forecasting_calendar to snowflake from:  https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/ts_demand_forecasting_calendar.csv\n"
     ]
    }
   ],
   "source": [
    "response = prepare_demo_tables_in_db(\n",
    "    db_user = db_user,                        \n",
    "    db_password = db_password,                \n",
    "    account = account,                        \n",
    "    db = db,                                  \n",
    "    warehouse = warehouse,                     \n",
    "    schema = schema\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eda26f8-04a2-4e57-a202-1cceaf893f8c",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b66a7ea9-550d-4b5c-8e9f-a37a2a751e02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.1\n",
      "Client version: 3.0.2\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "import pandas as pd\n",
    "import datarobot as dr\n",
    "\n",
    "import dr_utils as dru\n",
    "\n",
    "print('Python version:', python_version())\n",
    "print('Client version:', dr.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7349939a-1df8-455a-87c4-fb488cebc205",
   "metadata": {},
   "source": [
    "### Connect to DataRobot\n",
    "\n",
    "1. In DataRobot, navigate to **Developer Tools** by clicking on the user icon in the top-right corner. From here you can generate a API Key that you will use to authenticate to DataRobot. You can find more details on creating an API key [in the DataRobot documentation](https://app.datarobot.com/docs/api/api-quickstart/index.html#create-a-datarobot-api-key). \n",
    "\n",
    "2. Determine your DataRobot API Endpoint. The API endpoint is the same as your DataRobot UI root. Replace {datarobot.example.com} with your deployment endpoint.\n",
    "\n",
    "    API endpoint root: `https://{datarobot.example.com}/api/v2`\n",
    "    \n",
    "    For users of the AI Cloud platform, the endpoint is `https://app.datarobot.com/api/v2`\n",
    "    \n",
    "3. After obtaining your API Key and endpoint, there are several options to [connect to DataRobot](https://app.datarobot.com/docs/api/api-quickstart/index.html#configure-api-authentication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31780f68-12f1-4a3a-8d51-b32e23cb208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the DataRobot connection\n",
    "\n",
    "DATAROBOT_API_TOKEN = \"\" # Get this from the Developer Tools page in the DataRobot UI\n",
    "# Endpoint - This notebook uses the default endpoint for DataRobot Managed AI Cloud (US)\n",
    "DATAROBOT_ENDPOINT = \"https://app.datarobot.com/\" # This should be the URL you use to access the DataRobot UI\n",
    "\n",
    "client = dr.Client(\n",
    "    token=DATAROBOT_API_TOKEN, \n",
    "    endpoint=DATAROBOT_ENDPOINT,\n",
    "    user_agent_suffix='AIA-E2E-AWS-7' # Optional. Helps DataRobot improve this workflow\n",
    ")\n",
    "\n",
    "dr.client._global_client = client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe9b895-e301-47a7-b4c1-ec217abda2fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='data_prep'></a>\n",
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a8e50-f728-4881-8fa2-e268b393e780",
   "metadata": {},
   "source": [
    "### Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0e12bfb-1a18-4dc7-ac8f-74d250a9ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_col = 'DATE'\n",
    "series_id = 'STORE_SKU'\n",
    "target = 'UNITS'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8db9f44-9c04-4210-be34-934552b81221",
   "metadata": {},
   "source": [
    "### Configure a data connection\n",
    "\n",
    "DataRobot supports connections to a wide variety of databases through AI Catalog, allowing repeated access to the database as an AI Catalog data store. You can find the examples [in the DataRobot documentation](https://docs.datarobot.com/en/docs/data/connect-data/data-sources/index.html).\n",
    "\n",
    "[Credentials for the connection to your Data Store can be securely stored within DataRobot](https://docs.datarobot.com/en/docs/data/connect-data/stored-creds.html). They will be used during the dataset creation in the AI Catalog, and can be found under the `Data Connections` tab in DataRobot.  \n",
    "\n",
    "If you don't have credentials and a datastore created, uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a111f458-fb52-4e7c-b094-95d1e9b78665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the driver ID from name\n",
    "# Can be skipped if you have the ID - showing the code here for completeness\n",
    "# for d in dr.DataDriver.list():\n",
    "#     if d.canonical_name in 'Snowflake (3.13.9 - recommended)':\n",
    "#         print((d.id, d.canonical_name))\n",
    "        \n",
    "# # Create a datastore and datastore ID\n",
    "# data_store = dr.DataStore.create(data_store_type='jdbc', canonical_name='Snowflake Demo DB', driver_id='626bae0a98b54f9ba70b4122', jdbc_url= db_url)\n",
    "# data_store.test(username=db_user, password=db_password)\n",
    "\n",
    "# # Create and store credentials to allow the AI Catalog access to this database\n",
    "# # These can be found in the Data Connections tab under your profile in DataRobot  \n",
    "# cred = dr.Credential.create_basic(name='test_cred',user=db_user, password=db_password,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a98b9a2-fe44-43f0-9188-40fee018b6d9",
   "metadata": {},
   "source": [
    "Use the snippet below to find a credential and a data connection (AKA a data store)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aaf620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "creds_name = 'your_stored_credential'\n",
    "data_store_name = 'your_datastore_name'\n",
    "\n",
    "credential_id = [cr.credential_id for cr in dr.Credential.list() if cr.name == creds_name][0]\n",
    "data_store_id = [ds.id for ds in dr.DataStore.list() if ds.canonical_name == data_store_name][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c004bd-bd3f-4224-b05e-fc426573af38",
   "metadata": {},
   "source": [
    "Use the snippet below to create or get an existing data connection based on a query and upload a training dataset into the AI Catalog.\n",
    "\n",
    "You'll create two datasets with diffent training end dates to show how to set up a model retraining policies and track data and accuracy drift over time within DataRobot MLOps capabilities. Also, the last four weeks of data won't be used for training purposes at all in order to compare the performance of the previous and retrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58b64e2e-f03a-452d-8fed-012ff569cdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new data source: DataSource('ts_training_data_first')\n"
     ]
    }
   ],
   "source": [
    "date_train_max = \"'2022-05-09'\"\n",
    "\n",
    "data_source_train, dataset_train = dru.create_dataset_from_data_source(\n",
    "    data_source_name='ts_training_data_first', query=f'select * from {db}.{schema}.\"ts_demand_forecasting_train\" where {date_col} <= {date_train_max};',\n",
    "    data_store_id=data_store_id, credential_id=credential_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bde9d5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new data source: DataSource('ts_retraining_data')\n"
     ]
    }
   ],
   "source": [
    "date_train_max = \"'2022-09-26'\"\n",
    "\n",
    "data_source_retrain, dataset_retrain = dru.create_dataset_from_data_source(\n",
    "    data_source_name='ts_retraining_data', query=f'select * from {db}.{schema}.\"ts_demand_forecasting_train\" where {date_col} <= {date_train_max};',\n",
    "    data_store_id=data_store_id, credential_id=credential_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d31c9-b565-4f1f-b150-797316b23efa",
   "metadata": {},
   "source": [
    "Use the following snippet to create or get an existing data connection based on a query and upload a calendar dataset into the AI Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ba21ef5-aaa1-4b71-9ff5-d77ef0a66ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new data source: DataSource('ts_calendar')\n"
     ]
    }
   ],
   "source": [
    "data_source_calendar, dataset_calendar = dru.create_dataset_from_data_source(\n",
    "    data_source_name='ts_calendar', query=f'select * from {db}.{schema}.\"ts_demand_forecasting_calendar\";',\n",
    "    data_store_id=data_store_id, credential_id=credential_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57262668-8236-425e-892d-1c8a08982c2a",
   "metadata": {},
   "source": [
    "The following snippet is optional and can be used to get data to investigate before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b04abf-2a3a-46f2-9b4e-f7567d568cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the original data shape   : (7389, 13)\n",
      "the total number of series: 50\n",
      "the min date: 2019-05-06 00:00:00\n",
      "the max date: 2022-05-09 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORE_SKU</th>\n",
       "      <th>DATE</th>\n",
       "      <th>UNITS</th>\n",
       "      <th>UNITS_MIN</th>\n",
       "      <th>UNITS_MAX</th>\n",
       "      <th>UNITS_MEAN</th>\n",
       "      <th>UNITS_STD</th>\n",
       "      <th>TRANSACTIONS_SUM</th>\n",
       "      <th>PROMO_MAX</th>\n",
       "      <th>PRICE_MEAN</th>\n",
       "      <th>STORE</th>\n",
       "      <th>SKU</th>\n",
       "      <th>SKU_CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>store_130_SKU_120931082</td>\n",
       "      <td>2019-05-06</td>\n",
       "      <td>388.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>55.428571</td>\n",
       "      <td>8.182443</td>\n",
       "      <td>243.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.8</td>\n",
       "      <td>store_130</td>\n",
       "      <td>SKU_120931082</td>\n",
       "      <td>cat_1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>store_130_SKU_120931082</td>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>318.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>45.428571</td>\n",
       "      <td>8.079958</td>\n",
       "      <td>210.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.8</td>\n",
       "      <td>store_130</td>\n",
       "      <td>SKU_120931082</td>\n",
       "      <td>cat_1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>store_130_SKU_120931082</td>\n",
       "      <td>2019-05-20</td>\n",
       "      <td>126.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>3.915780</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.8</td>\n",
       "      <td>store_130</td>\n",
       "      <td>SKU_120931082</td>\n",
       "      <td>cat_1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>store_130_SKU_120931082</td>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>285.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>40.714286</td>\n",
       "      <td>14.067863</td>\n",
       "      <td>197.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.8</td>\n",
       "      <td>store_130</td>\n",
       "      <td>SKU_120931082</td>\n",
       "      <td>cat_1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>store_130_SKU_120931082</td>\n",
       "      <td>2019-06-03</td>\n",
       "      <td>93.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.285714</td>\n",
       "      <td>3.352327</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.8</td>\n",
       "      <td>store_130</td>\n",
       "      <td>SKU_120931082</td>\n",
       "      <td>cat_1160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 STORE_SKU       DATE  UNITS  UNITS_MIN  UNITS_MAX  \\\n",
       "0  store_130_SKU_120931082 2019-05-06  388.0       44.0       69.0   \n",
       "1  store_130_SKU_120931082 2019-05-13  318.0       37.0       62.0   \n",
       "2  store_130_SKU_120931082 2019-05-20  126.0       13.0       23.0   \n",
       "3  store_130_SKU_120931082 2019-05-27  285.0       23.0       65.0   \n",
       "4  store_130_SKU_120931082 2019-06-03   93.0       10.0       20.0   \n",
       "\n",
       "   UNITS_MEAN  UNITS_STD  TRANSACTIONS_SUM  PROMO_MAX  PRICE_MEAN      STORE  \\\n",
       "0   55.428571   8.182443             243.0        1.0        44.8  store_130   \n",
       "1   45.428571   8.079958             210.0        1.0        44.8  store_130   \n",
       "2   18.000000   3.915780             118.0        0.0        44.8  store_130   \n",
       "3   40.714286  14.067863             197.0        1.0        44.8  store_130   \n",
       "4   13.285714   3.352327              87.0        0.0        44.8  store_130   \n",
       "\n",
       "             SKU SKU_CATEGORY  \n",
       "0  SKU_120931082     cat_1160  \n",
       "1  SKU_120931082     cat_1160  \n",
       "2  SKU_120931082     cat_1160  \n",
       "3  SKU_120931082     cat_1160  \n",
       "4  SKU_120931082     cat_1160  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = dataset_train.get_as_dataframe()\n",
    "df_train[date_col] = pd.to_datetime(df_train[date_col], format='%Y-%m-%d')\n",
    "print('the original data shape   :', df_train.shape)\n",
    "print('the total number of series:', df_train[series_id].nunique())\n",
    "print('the min date:', df_train[date_col].min())\n",
    "print('the max date:', df_train[date_col].max())\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d06d558f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the original data shape   : (8389, 13)\n",
      "the total number of series: 50\n",
      "the min date: 2019-05-06 00:00:00\n",
      "the max date: 2022-09-26 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORE_SKU</th>\n",
       "      <th>DATE</th>\n",
       "      <th>UNITS</th>\n",
       "      <th>UNITS_MIN</th>\n",
       "      <th>UNITS_MAX</th>\n",
       "      <th>UNITS_MEAN</th>\n",
       "      <th>UNITS_STD</th>\n",
       "      <th>TRANSACTIONS_SUM</th>\n",
       "      <th>PROMO_MAX</th>\n",
       "      <th>PRICE_MEAN</th>\n",
       "      <th>STORE</th>\n",
       "      <th>SKU</th>\n",
       "      <th>SKU_CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>store_130_SKU_120931082</td>\n",
       "      <td>2019-05-06</td>\n",
       "      <td>388.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>55.428571</td>\n",
       "      <td>8.182443</td>\n",
       "      <td>243.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.8</td>\n",
       "      <td>store_130</td>\n",
       "      <td>SKU_120931082</td>\n",
       "      <td>cat_1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>store_130_SKU_120931082</td>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>318.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>45.428571</td>\n",
       "      <td>8.079958</td>\n",
       "      <td>210.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.8</td>\n",
       "      <td>store_130</td>\n",
       "      <td>SKU_120931082</td>\n",
       "      <td>cat_1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>store_130_SKU_120931082</td>\n",
       "      <td>2019-05-20</td>\n",
       "      <td>126.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>3.915780</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.8</td>\n",
       "      <td>store_130</td>\n",
       "      <td>SKU_120931082</td>\n",
       "      <td>cat_1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>store_130_SKU_120931082</td>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>285.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>40.714286</td>\n",
       "      <td>14.067863</td>\n",
       "      <td>197.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.8</td>\n",
       "      <td>store_130</td>\n",
       "      <td>SKU_120931082</td>\n",
       "      <td>cat_1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>store_130_SKU_120931082</td>\n",
       "      <td>2019-06-03</td>\n",
       "      <td>93.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.285714</td>\n",
       "      <td>3.352327</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.8</td>\n",
       "      <td>store_130</td>\n",
       "      <td>SKU_120931082</td>\n",
       "      <td>cat_1160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 STORE_SKU       DATE  UNITS  UNITS_MIN  UNITS_MAX  \\\n",
       "0  store_130_SKU_120931082 2019-05-06  388.0       44.0       69.0   \n",
       "1  store_130_SKU_120931082 2019-05-13  318.0       37.0       62.0   \n",
       "2  store_130_SKU_120931082 2019-05-20  126.0       13.0       23.0   \n",
       "3  store_130_SKU_120931082 2019-05-27  285.0       23.0       65.0   \n",
       "4  store_130_SKU_120931082 2019-06-03   93.0       10.0       20.0   \n",
       "\n",
       "   UNITS_MEAN  UNITS_STD  TRANSACTIONS_SUM  PROMO_MAX  PRICE_MEAN      STORE  \\\n",
       "0   55.428571   8.182443             243.0        1.0        44.8  store_130   \n",
       "1   45.428571   8.079958             210.0        1.0        44.8  store_130   \n",
       "2   18.000000   3.915780             118.0        0.0        44.8  store_130   \n",
       "3   40.714286  14.067863             197.0        1.0        44.8  store_130   \n",
       "4   13.285714   3.352327              87.0        0.0        44.8  store_130   \n",
       "\n",
       "             SKU SKU_CATEGORY  \n",
       "0  SKU_120931082     cat_1160  \n",
       "1  SKU_120931082     cat_1160  \n",
       "2  SKU_120931082     cat_1160  \n",
       "3  SKU_120931082     cat_1160  \n",
       "4  SKU_120931082     cat_1160  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_retrain = dataset_retrain.get_as_dataframe()\n",
    "df_retrain[date_col] = pd.to_datetime(df_retrain[date_col], format='%Y-%m-%d')\n",
    "print('the original data shape   :', df_retrain.shape)\n",
    "print('the total number of series:', df_retrain[series_id].nunique())\n",
    "print('the min date:', df_retrain[date_col].min())\n",
    "print('the max date:', df_retrain[date_col].max())\n",
    "df_retrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9da601-abc0-4556-8995-823b0a729b44",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='modeling'></a>\n",
    "## Modeling\n",
    "\n",
    "The next step after the data preparation and before modeling is [to specify the modeling parameters](https://datarobot-public-api-client.readthedocs-hosted.com/en/v2.28.0/reference/modeling/spec/time_series.html#):\n",
    "\n",
    "- *Features known in advance* are things you know in the future, such as product metadata or a planned marketing event. If all features are known in advance, use the setting `default_to_known_in_advance`.\n",
    "\n",
    "- *Do not derive features* will be excluded from deriving time-related features. If all features should be excluded, `default_to_do_not_derive` can be used.\n",
    "\n",
    "- `metric` is used for evaluating models. [DataRobot supports a wide variety of metrics](https://app.datarobot.com/docs/modeling/reference/model-detail/opt-metric.html). The metric used depends on the use case. If the value is not specified, DataRobot suggests a metric based on the target distribution.\n",
    "\n",
    "- `feature_derivation_window_start` and `feature_derivation_window_end` define the feature derivation window (FDW). The FDW represents the rolling window that is used to derive time series features and lags. FDW definition should be long enough to capture relevant trends to your use case. On the other hand, FDW shouldn't be too long (e.g., 365 days) because it shrinks the available training data and increases the size of the feature list. Older data does not help the model learn recent trends. It is not necessary to have a year-long FDW to capture seasonality; DataRobot auto-derives features (month indicator, day indicator), as well as learns effects near calendar events (if a calendar is provided), in addition to blueprint specific techniques for seasonality.\n",
    "\n",
    "- `gap_duration` is the duration of the gap between training and validation or holdout scoring data, representing delays in data availability. For example, at prediction time, if events occuring on Monday aren't reported or made available until Wednesday, you would have a gap of two days. This can occur with reporting lags or with data that requires some form of validation before being stored on a system of record.\n",
    "\n",
    "- `forecast_window_start` and `forecast_window_end` defines the forecast window (FW). It represents the rolling window of future values to predict. FW depends on a business application of the model predictions.\n",
    "\n",
    "- `number_of_backtests` and `validation_duration`. Proper backtest configuration helps evaluate the model’s ability to generalize to the appropriate time periods. The main considerations during the backtests specification are listed below.\n",
    "    - Validation from all backtests combined should span the region of interest.\n",
    "    - Fewer backtests means the validation lengths might need to be longer.\n",
    "    - After the specification of the appropriate validation length, the number of backtests should be adjusted until they span a full region of interest.\n",
    "    - The validation duration should be at least as long as your best estimate of the amount of time the model will be in production without retraining.\n",
    "\n",
    "- `holdout_start_date` with one of `holdout_end_date` or `holdout_duration` can be added additionally. DataRobot will define them based on `validation_duration` if they were not specified.\n",
    "\n",
    "- `calendar_id` is the ID of the previously created calendar. DataRobot automatically create features based on the calendar events (such as “days to next event”). There are several options to create a calendar:\n",
    "    \n",
    "    - From an AI Catalog dataset:\n",
    "        ```python\n",
    "        calendar = dr.CalendarFile.create_calendar_from_dataset(dataset_id)\n",
    "        \n",
    "        \n",
    "        ```\n",
    "    - Based on the provided country code and dataset start date and end dates:\n",
    "        ```python\n",
    "        calendar = dr.CalendarFile.create_calendar_from_country_code(country_code, start_date, end_date)\n",
    "        \n",
    "        \n",
    "        ```\n",
    "    - From a local file:\n",
    "        ```python\n",
    "        calendar = dr.CalendarFile.create(path_to_calendar_file)\n",
    "        \n",
    "        \n",
    "        ```\n",
    "\n",
    "- `allow_partial_history_time_series_predictions` - Not all blueprints are designed to predict on new series with only partial history, as it can lead to suboptimal predictions. This is because for those blueprints the full history is needed to derive the features for specific forecast points. \"Cold start\" is the ability to model on series that were not seen in the training data; partial history refers to prediction datasets with series history that is only partially known (historical rows are partially available within the feature derivation window). If `True`, Autopilot runs the blueprints optimized for cold start and also for partial history modeling, eliminating models with less accurate results for partial history support.\n",
    "\n",
    "- `segmented_project` set to `True` and the cluster name `cluster_id` can be used for segmented modeling. This feature offers the ability to build multiple forecasting models simultaneously. DataRobot creates multiple projects “under the hood”. Each project is specific to its own data per `cluster_id`. The model benefits by having forecasts tailored to the specific data subset, rather than assuming that the important features are going to be the same across all of series. The models for different `cluster_id`s will have features engineered specifically from cluster-specific data. The benefits of segmented modeling also extend to deployments. Rather than deploying each model separately, you can deploy all of them at once within one segmented deployment.\n",
    "\n",
    "> The function `dr.helpers.partitioning_methods.construct_duration_string()` can be used to construct a valid string representing the `gap_duration`, `validation_duration` and `holdout_duration` duration in accordance with ISO8601."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f5d7c0-5710-4c1e-a312-8a9efa3beabc",
   "metadata": {},
   "source": [
    "### Create a calendar\n",
    "\n",
    "Create a calendar based on the dataset in the AI Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "860d974d-dd77-4a91-97b1-f9f7c6db18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = dr.CalendarFile.create_calendar_from_dataset(dataset_id=dataset_calendar.id, calendar_name=dataset_calendar.name)\n",
    "calendar_id = calendar.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbc8aea-6f76-4c43-9a9a-5d643534eda7",
   "metadata": {},
   "source": [
    "### Configure modeling settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40f1c744-83c9-4764-a304-0a976c4eb140",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_known_in_advance = ['STORE', 'SKU', 'SKU_CATEGORY', 'PROMO_MAX']\n",
    "do_not_derive_features = ['STORE', 'SKU', 'SKU_CATEGORY']\n",
    "\n",
    "params = {\n",
    "    'metric': None,\n",
    "    'features_known_in_advance': features_known_in_advance,\n",
    "    'do_not_derive_features': do_not_derive_features,\n",
    "    \n",
    "    'target': target,\n",
    "    'mode': 'quick',\n",
    "    \n",
    "    'datetime_partition_column': date_col,\n",
    "    'multiseries_id_columns': [series_id],\n",
    "    'use_time_series': True,\n",
    "    \n",
    "    'feature_derivation_window_start': None,\n",
    "    'feature_derivation_window_end': None,\n",
    "    \n",
    "    'gap_duration': None,\n",
    "    \n",
    "    'forecast_window_start': None,\n",
    "    'forecast_window_end': None,\n",
    "    \n",
    "    'number_of_backtests': None,\n",
    "    'validation_duration': None,\n",
    "    \n",
    "    'calendar_id': calendar_id,\n",
    "    \n",
    "    'allow_partial_history_time_series_predictions': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f8444e-36f8-4f02-a022-6da103c54430",
   "metadata": {},
   "source": [
    "### Create and run projects\n",
    "\n",
    "In order to try all cold start solution methods, run two projects on the same training data. \n",
    "\n",
    "Use the following snippet to create and run the project without new series support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c102558d-14c9-4e1d-beda-f310630881b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataRobot will define FDW and FD automatically.\n",
      "2023-03-27 11:21:16.314331 start: UNITS_20230327_1121\n"
     ]
    }
   ],
   "source": [
    "project = dru.run_project(data=dataset_train, params=params)\n",
    "project.wait_for_autopilot(verbosity=dr.enums.VERBOSITY_LEVEL.SILENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3675dbc8-b1b6-417e-af7e-576dd054894e",
   "metadata": {},
   "source": [
    "<a id='deployment'></a>\n",
    "## Deployment\n",
    "\n",
    "You have multiple options for a production deployment in DataRobot MLOps. Creating a deployment adds your model package to the Model Registry and containerizes all model artifacts, generates compliance documentation, exposes a production quality REST API on a prediction server in your DataRobot cluster, and enables all lifecycle management functionality, like drift monitoring.\n",
    "\n",
    "`dru.make_deployment` can be used to deploy a specific model by passing the `model_id`. If `model_id` is **None**, the DataRobot recommended model from the project is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3c9e630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment ID: 64215a1adecb97f050eccafd; URL: https://app.datarobot.com/deployments/64215a1adecb97f050eccafd/overview\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deployment = dru.make_deployment(project, target_drift_enabled=True, feature_drift_enabled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d850af5",
   "metadata": {},
   "source": [
    "Update your deployment's additional settings in order to set up automated reatraining later:\n",
    "\n",
    "- `update_association_id_settings`: The association ID functions as a foreign key for your prediction dataset so you can later match up actuals with those predictions. It corresponds to an event for which you want to track the outcome. In this case, combine **series ID** and the corresponding **forecast date**.\n",
    "\n",
    "- `update_predictions_data_collection_settings` and `update_challenger_models_settings`: These settings allow predictions to be scored on selected models in order to compare the performance with the curently deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be0e71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.update_association_id_settings(column_names=[f'{series_id}_{date_col}'], required_in_prediction_requests=True)\n",
    "deployment.update_predictions_data_collection_settings(enabled=True)\n",
    "deployment.update_challenger_models_settings(challenger_models_enabled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66f988e",
   "metadata": {},
   "source": [
    "Additionally, use the DataRobot UI to enable the option to infer actual values from time series history and automatically use them for accuracy estimation.\n",
    "\n",
    "<img src=\"images/auto_feedback.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b837f6ef",
   "metadata": {},
   "source": [
    "<a id='retr'></a>\n",
    "## Set up automatatic retraining\n",
    "\n",
    "To maintain model performance after deployment without extensive manual work, DataRobot provides an automatic retraining capability for deployments. Upon providing a retraining dataset registered in the **AI Catalog**, you can define up to five retraining policies on each deployment, each consisting of a trigger, a modeling strategy, modeling settings, and a replacement action. When triggered, retraining will produce a new model based on these settings and notify you to consider promoting it.\n",
    "\n",
    "You can find more details on how to set up automated retraining [in the DataRobot documentation](https://app.datarobot.com/docs/mlops/manage-mlops/set-up-auto-retraining.html#set-up-retraining-for-a-deployment)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec378457",
   "metadata": {},
   "source": [
    "### Provide retraining data\n",
    "\n",
    "All retraining policies on a deployment refer to the same **AI Catalog** dataset. You can register the dataset by navigating to the **Settings > Data** tab of the deployment and adding it to the **Learning** section. Alternatively, you can add training data directly from the **Challengers and Retraining** tab.\n",
    "<img src=\"images/reatraining_data.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baed11a",
   "metadata": {},
   "source": [
    "### Schedule monitoring\n",
    "\n",
    "DataRobot provides automated monitoring with a notification system. You can configure notifications to alert you when the service health, data drift status, model accuracy, or fairness exceed your defined acceptable levels.\n",
    "\n",
    "#### [Set up data drift monitoring](https://app.datarobot.com/docs/mlops/governance/deploy-notifications.html#set-up-data-drift-monitoring)\n",
    "Drift assesses how the distribution of data changes across all features, for a specified range. The thresholds you set determine the amount of drift you will allow before a notification is triggered.\n",
    "\n",
    "Use the **Data Drift** section of the **Monitoring** tab to set thresholds for drift and importance:\n",
    "\n",
    "- Drift is a measure of how new prediction data differs from the original data used to train the model.\n",
    "- Importance allows you to separate the features you care most about from those that are less important.\n",
    "\n",
    "For both drift and importance, you can visualize the thresholds and how they separate the features on the [Data Drift tab](https://app.datarobot.com/docs/mlops/monitor/data-drift.html).\n",
    "\n",
    "#### [Set up accuracy monitoring](https://app.datarobot.com/docs/mlops/governance/deploy-notifications.html#set-up-accuracy-monitoring)\n",
    "\n",
    "For Accuracy, the notification conditions relate to a performance optimization metric for the underlying model in the deployment. Select from the same set of metrics that are available on the Leaderboard. You can visualize accuracy using the [Accuracy over Time graph](https://app.datarobot.com/docs/mlops/monitor/deploy-accuracy.html#accuracy-over-time-graph) and the [Prediction & Actual graph](https://app.datarobot.com/docs/mlops/monitor/deploy-accuracy.html#predicted-actual-graph).\n",
    "\n",
    "Accuracy monitoring is defined by a single accuracy rule. Every 30 seconds, the rule evaluates the deployment's accuracy. Notifications trigger when this rule is violated.\n",
    "\n",
    "Prior to configuring accuracy notifications and monitoring for a deployment, set an [association ID](https://app.datarobot.com/docs/mlops/manage-mlops/setup-accuracy.html#select-an-association-id).\n",
    "\n",
    "<img src=\"images/monitoring.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e066bc8e",
   "metadata": {},
   "source": [
    "### Set up retraining policies\n",
    "\n",
    "1. On the **Settings > Challengers and Retraining** tab for a deployment, click **+ Add Retraining Policy**.\n",
    "<img src=\"images/retrain_add.png\"/>\n",
    "\n",
    "2. Set a [retraining trigger](https://app.datarobot.com/docs/mlops/manage-mlops/set-up-auto-retraining.html#triggers).\n",
    "\n",
    "3. Configure how DataRobot [selects a model](https://app.datarobot.com/docs/mlops/manage-mlops/set-up-auto-retraining.html#time-series-model-selection) from the new Autopilot project.\n",
    "\n",
    "4. Set up a replacement strategy by selecting a [model action](https://app.datarobot.com/docs/mlops/manage-mlops/set-up-auto-retraining.html#model-action).\n",
    "\n",
    "5. Set up a [modeling strategy](https://app.datarobot.com/docs/mlops/manage-mlops/set-up-auto-retraining.html#time-series-modeling-strategy) by selecting settings for the new Autopilot project.\n",
    "\n",
    "6. Click **Save policy** above the policy settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911249b1",
   "metadata": {},
   "source": [
    "#### Triggers\n",
    "Retraining policies can be triggered manually or in response to three types of conditions:\n",
    "\n",
    "- **Automatic schedule**: Pick a time for the retraining policy to trigger automatically. Choose from increments ranging from every three months to every day. Note that DataRobot uses your local time zone.\n",
    "\n",
    "- **Drift status**: Initiates retraining when the deployment's data drift status declines to the level(s) you select.\n",
    "\n",
    "- **Accuracy status**: Triggers when the deployment's accuracy status changes from a better status to the levels you select (green to yellow, yellow to red, etc.).\n",
    "\n",
    "#### Model selection\n",
    "\n",
    "**Same blueprint as champion**: The retraining policy uses the same engineered features as the champion model's blueprint. The search for newly derived features does not occur because it could potentially generate features that are not captured in the champion's blueprint.\n",
    "\n",
    "**Autopilot**: When using Autopilot instead of the same blueprint, the time series feature derivation process does occur. However, Comprehensive Autopilot mode is not supported. Additionally, time series Autopilot does not support the options to only include Scoring Code blueprints and models with SHAP value support.\n",
    "\n",
    "<img src=\"images/model_selection.png\"/>\n",
    "\n",
    "#### Model action\n",
    "\n",
    "<img src=\"images/model_action.png\"/>\n",
    "\n",
    "#### Modeling strategy\n",
    "\n",
    "**Same blueprint as champion**: When creating a \"same-blueprint\" retraining policy for a time series deployment, you must use the champion model's feature list and advanced modeling options. The only option that you can override is the calendar used because, for example, a new holiday or event may be included in an updated calendar that you want to account for during retraining.\n",
    "\n",
    "**Autopilot**: When creating an Autopilot retraining policy for a time series deployment, you must use the informative features modeling strategy. This strategy allows Autopilot to derive a new set of feature lists based on the informative features generated by new or different data. You cannot use the model's original feature list because time series Autopilot uses a feature extraction and reduction process by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cceede7-cb37-406e-a585-5c435db9bcb2",
   "metadata": {},
   "source": [
    "<a id='preds'></a>\n",
    "## Make predictions\n",
    "\n",
    "Once you have set up model retraining, use your deoloyment to make predictions for several consecutive months to track data and accuracy drift over time.\n",
    "\n",
    "The scoring dataset should follow requirements to ensure the Batch Prediction API can make predictions:\n",
    "\n",
    "- Sort prediction rows by their series ID then timestamp, with the earliest row first.\n",
    "- The dataset must contain rows without a target for the desired forecast window.\n",
    "\n",
    "You can find more details on the scoring dataset structure [in the DataRobot documentation](https://app.datarobot.com/docs/api/reference/batch-prediction-api/batch-pred-ts.html#requirements-for-the-scoring-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8017f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_points = [\"'2022-05-09'\", \"'2022-06-06'\", \"'2022-07-04'\", \"'2022-08-01'\", \"'2022-08-29'\", \"'2022-09-26'\"]\n",
    "\n",
    "model_name = \"'first model'\"\n",
    "\n",
    "for forecast_point in forecast_points:\n",
    "    query = f\"\"\"\n",
    "        select\n",
    "            concat(t.store_sku, '_', t.date::varchar) store_sku_date,\n",
    "            t.store_sku,\n",
    "            t.date,\n",
    "            iff(t.date > {forecast_point}, null, t.units) units,\n",
    "            iff(t.date > {forecast_point}, null, t.units_min) units_min,\n",
    "            iff(t.date > {forecast_point}, null, t.units_max) units_max,\n",
    "            iff(t.date > {forecast_point}, null, t.units_mean) units_mean,\n",
    "            iff(t.date > {forecast_point}, null, t.units_std) units_std,\n",
    "            iff(t.date > {forecast_point}, null, t.transactions_sum) transactions_sum,\n",
    "            t.promo_max,\n",
    "            iff(t.date > {forecast_point}, null, t.price_mean) price_mean,\n",
    "            t.store,\n",
    "            t.sku,\n",
    "            t.sku_category,\n",
    "            {model_name} model_name\n",
    "        from {db}.{schema}.\"ts_demand_forecasting_train\" t\n",
    "        where\n",
    "            t.date >= dateadd(day, -70, {forecast_point}) and \n",
    "            t.date <= dateadd(day,  28, {forecast_point})\n",
    "        order by\n",
    "            t.store_sku,\n",
    "            t.date\n",
    "        ;\n",
    "    \"\"\"\n",
    "\n",
    "    intake_settings = {\n",
    "        \"type\": \"jdbc\",\n",
    "        \"data_store_id\": data_store_id,\n",
    "        \"credential_id\": credential_id,\n",
    "        \"query\": query\n",
    "        }\n",
    "\n",
    "    output_settings = {\n",
    "        \"type\": \"jdbc\",\n",
    "        \"data_store_id\": data_store_id,\n",
    "        \"credential_id\": credential_id,\n",
    "        \"table\": \"ts_demand_forecasting_predictions\",\n",
    "        \"schema\": schema,\n",
    "        \"catalog\": db,\n",
    "        \"create_table_if_not_exists\": True,\n",
    "        \"statement_type\": \"insert\"\n",
    "        }\n",
    "\n",
    "    pred_job = dru.make_predictions_from_deployment(deployment=deployment, intake_settings=intake_settings, output_settings=output_settings,\n",
    "                                    passthrough_columns_set='all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04ceeec",
   "metadata": {},
   "source": [
    "### Data drift\n",
    "\n",
    "The plot below shows the data drift for a deployment. Currently, only two features with relative low importance have drifted significantly.\n",
    "<img src=\"images/data_drift.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1687766",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "According to accuracy tracking the model performance degraded over those several months scoring period. RMSE is more than 22% worse compared to the original model's performance. It will trigger the corresponding retraining policy.\n",
    "\n",
    "<img src=\"images/accuracy.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdd05dd",
   "metadata": {},
   "source": [
    "### Predictions after retraining\n",
    "\n",
    "To check the retraining benefits, compare the first and retraining models performance on the period right after the training data ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f041a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_points = [\"'2022-09-26'\"]\n",
    "\n",
    "model_name = \"'retrained model'\"\n",
    "\n",
    "for forecast_point in forecast_points:\n",
    "    query = f\"\"\"\n",
    "        select\n",
    "            concat(t.store_sku, '_', t.date::varchar) store_sku_date,\n",
    "            t.store_sku,\n",
    "            t.date,\n",
    "            iff(t.date > {forecast_point}, null, t.units) units,\n",
    "            iff(t.date > {forecast_point}, null, t.units_min) units_min,\n",
    "            iff(t.date > {forecast_point}, null, t.units_max) units_max,\n",
    "            iff(t.date > {forecast_point}, null, t.units_mean) units_mean,\n",
    "            iff(t.date > {forecast_point}, null, t.units_std) units_std,\n",
    "            iff(t.date > {forecast_point}, null, t.transactions_sum) transactions_sum,\n",
    "            t.promo_max,\n",
    "            iff(t.date > {forecast_point}, null, t.price_mean) price_mean,\n",
    "            t.store,\n",
    "            t.sku,\n",
    "            t.sku_category,\n",
    "            {model_name} model_name\n",
    "        from {db}.{schema}.\"ts_demand_forecasting_train\" t\n",
    "        where\n",
    "            t.date >= dateadd(day, -70, {forecast_point}) and \n",
    "            t.date <= dateadd(day,  28, {forecast_point})\n",
    "        order by\n",
    "            t.store_sku,\n",
    "            t.date\n",
    "        ;\n",
    "    \"\"\"\n",
    "\n",
    "    intake_settings = {\n",
    "        \"type\": \"jdbc\",\n",
    "        \"data_store_id\": data_store_id,\n",
    "        \"credential_id\": credential_id,\n",
    "        \"query\": query\n",
    "        }\n",
    "\n",
    "    output_settings = {\n",
    "        \"type\": \"jdbc\",\n",
    "        \"data_store_id\": data_store_id,\n",
    "        \"credential_id\": credential_id,\n",
    "        \"table\": \"ts_demand_forecasting_predictions\",\n",
    "        \"schema\": schema,\n",
    "        \"catalog\": db,\n",
    "        \"create_table_if_not_exists\": True,\n",
    "        \"statement_type\": \"insert\"\n",
    "        }\n",
    "\n",
    "    pred_job = dru.make_predictions_from_deployment(deployment=deployment, intake_settings=intake_settings, output_settings=output_settings,\n",
    "                                    passthrough_columns_set='all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d010f98-0fb7-4747-aa1c-aee532c66ef2",
   "metadata": {},
   "source": [
    "## Get predictions\n",
    "\n",
    "In the previous step, you ran prediction jobs from your registered data store (Snowflake) on the deployment before and after retraining. The records with the value `first model` in the columns `model_name` are predictions before the model retraining. They cover longer period in order to check data drift and accuracy over time. Since you've observed a significant drop in the model performance, the retraining was triggered. The records with the value `retrained model` in the columns `model_name` were made on the data right after the retraining data ends.\n",
    "\n",
    "The steps below will add the new prediction table, `ts_demand_forecasting_predictions_combined`, to the AI Catalog. This enables versioning of each prediction run, and completing the comparison of the model before and after retraining. It contains the predictions for the period right after the retraining data ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec50667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "    select\n",
    "        t.store_sku,\n",
    "        t.date,\n",
    "        t.units,\n",
    "        pf.\"UNITS (actual)_PREDICTION\" prediction_first_model,\n",
    "        pr.\"UNITS (actual)_PREDICTION\" prediction_retrained_model\n",
    "    from {db}.{schema}.\"ts_demand_forecasting_train\" t\n",
    "    join {db}.{schema}.\"ts_demand_forecasting_predictions\" pf\n",
    "        on t.store_sku = pf.store_sku\n",
    "        and t.date = pf.date\n",
    "        and pf.model_name = 'first model'\n",
    "    join {db}.{schema}.\"ts_demand_forecasting_predictions\" pr\n",
    "        on t.store_sku = pr.store_sku\n",
    "        and t.date = pr.date\n",
    "        and pr.model_name = 'retrained model'\n",
    "    ;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fdc6427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new data source: DataSource('ts_demand_forecasting_predictions_combined')\n"
     ]
    }
   ],
   "source": [
    "# Create or get an existing data connection based on a query and upload a training dataset into the AI Catalog\n",
    "\n",
    "data_source_preds, dataset_preds = dru.create_dataset_from_data_source(\n",
    "    data_source_name='ts_demand_forecasting_predictions_combined', query=query,\n",
    "    data_store_id=data_store_id, credential_id=credential_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "472b10a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the original data shape   : (200, 5)\n",
      "the total number of series: 50\n",
      "the min date: 2022-10-03 00:00:00\n",
      "the max date: 2022-10-24 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORE_SKU</th>\n",
       "      <th>DATE</th>\n",
       "      <th>UNITS</th>\n",
       "      <th>PREDICTION_FIRST_MODEL</th>\n",
       "      <th>PREDICTION_RETRAINED_MODEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>store_130_SKU_120931082</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>86.0</td>\n",
       "      <td>73.832690</td>\n",
       "      <td>73.985825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>store_130_SKU_120931082</td>\n",
       "      <td>2022-10-10</td>\n",
       "      <td>97.0</td>\n",
       "      <td>74.379489</td>\n",
       "      <td>73.380730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>store_130_SKU_120931082</td>\n",
       "      <td>2022-10-17</td>\n",
       "      <td>73.0</td>\n",
       "      <td>72.948390</td>\n",
       "      <td>72.261116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>store_130_SKU_120931082</td>\n",
       "      <td>2022-10-24</td>\n",
       "      <td>64.0</td>\n",
       "      <td>71.079112</td>\n",
       "      <td>71.776375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>store_130_SKU_120969795</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>63.0</td>\n",
       "      <td>52.748136</td>\n",
       "      <td>52.485294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 STORE_SKU       DATE  UNITS  PREDICTION_FIRST_MODEL  \\\n",
       "0  store_130_SKU_120931082 2022-10-03   86.0               73.832690   \n",
       "1  store_130_SKU_120931082 2022-10-10   97.0               74.379489   \n",
       "2  store_130_SKU_120931082 2022-10-17   73.0               72.948390   \n",
       "3  store_130_SKU_120931082 2022-10-24   64.0               71.079112   \n",
       "4  store_130_SKU_120969795 2022-10-03   63.0               52.748136   \n",
       "\n",
       "   PREDICTION_RETRAINED_MODEL  \n",
       "0                   73.985825  \n",
       "1                   73.380730  \n",
       "2                   72.261116  \n",
       "3                   71.776375  \n",
       "4                   52.485294  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get predictions as a DataFrame\n",
    "\n",
    "df_preds = dataset_preds.get_as_dataframe()\n",
    "df_preds[date_col] = pd.to_datetime(df_preds[date_col], format='%Y-%m-%d')\n",
    "df_preds.sort_values([series_id, date_col], inplace=True)\n",
    "print('the original data shape   :', df_preds.shape)\n",
    "print('the total number of series:', df_preds[series_id].nunique())\n",
    "print('the min date:', df_preds[date_col].min())\n",
    "print('the max date:', df_preds[date_col].max())\n",
    "df_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336662e",
   "metadata": {},
   "source": [
    "The overlapping period contains predictions for four consecutive weeks. The retrained model has significantly better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fbb6a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE before retraining: 41.95933587139678\n",
      "RMSE after retraining : 34.24508060550132\n"
     ]
    }
   ],
   "source": [
    "print('RMSE before retraining:', dru.rmse(df_preds, target, 'PREDICTION_FIRST_MODEL'))\n",
    "print('RMSE after retraining :', dru.rmse(df_preds, target, 'PREDICTION_RETRAINED_MODEL'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26423b0e-6050-4ef4-b7ef-1b8d768f6840",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook's workflow provides a repeatable framework from project setup to model deployment, monitoring, and retraining for times series data with multiple series (SKUs) with full, partial and no history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f5d31-a590-4cc4-b34d-252f1edd1f04",
   "metadata": {},
   "source": [
    "## Delete project artifacts \n",
    "\n",
    "Optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a24aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment and run this cell to remove everything you added in DataRobot during this session\n",
    "\n",
    "# dr.Dataset.delete(dataset_train.id)\n",
    "# dr.Dataset.delete(dataset_retrain.id)\n",
    "# dr.Dataset.delete(dataset_calendar.id)\n",
    "# dr.Dataset.delete(dataset_preds.id)\n",
    "\n",
    "# data_source_train.delete()\n",
    "# data_source_retrain.delete()\n",
    "# data_source_calendar.delete()\n",
    "# data_source_preds.delete()\n",
    "\n",
    "# deployment.delete()\n",
    "\n",
    "# project.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
