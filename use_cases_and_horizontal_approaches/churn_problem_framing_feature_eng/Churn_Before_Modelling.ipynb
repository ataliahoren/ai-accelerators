{
 "cells": [
  {
   "cell_type": "raw",
   "id": "236458a1-9a0b-4c95-b5da-14771cb1fe09",
   "metadata": {},
   "source": [
    "---\n",
    "title: Tackling Churn with ML - Before Modelling\n",
    "theme: cosmo\n",
    "execute:\n",
    "  enable : false\n",
    "  eval: false\n",
    "  echo: true\n",
    "format:\n",
    "  html: \n",
    "    code-fold: true\n",
    "    code-summary: Show code\n",
    "    code-tools: true\n",
    "    callout-icon: false\n",
    "    callout-appearance: minimal\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d945d2",
   "metadata": {},
   "source": [
    "Authors: Max Dugan-Knight, Jake Snyder\\\n",
    "Date: 3/7/2023\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Customer retention is central to any successful business and machine learning is frequently proposed as a way of addressing churn. It is tempting to dive right into a churn dataset, but improving outcomes requires correctly framing the problem. Doing so at the start will determine whether the business can take action based on the trained model and whether your hard work is valuable or not.\n",
    "\n",
    "This blog will teach the problem framing and data management steps required before modelling begins. We will use two examples to illustrate concepts: a B2C retail example and a B2B example based on DataRobot’s internal churn model. \n",
    "\n",
    "One of the fundamental misconceptions about modelling churn is that a good churn model will reduce churn. Even an excellent model will have no impact on churn by itself. It will just correctly identify at risk customers. It is the consumers of the model's predictions who take action to retain customers. In fact, if a churn model perfectly predicts which customers will leave, it means the interventions had no impact on customer retention.\n",
    "\n",
    "Sometimes these interventions can be automated, like triggering an email with a discount. Often it is a person who decides whether and how to intervene. This means that as we build a churn model, we need our end users to trust the model and consider its recommendations in their actions. Keeping our end users in mind is a theme that will be present throughout our blog series as we demonstrate how to build a useful churn model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7119f164",
   "metadata": {},
   "source": [
    "# Problem Framing\n",
    "\n",
    "To frame this problem, we need to identify stakeholders, create three business definitions, and then decide on the consumption plan. Stakeholders will help with every section, which is why we need to identify them first. \n",
    "\n",
    "\n",
    "## Identify stakeholders\n",
    "\n",
    "Because our end users are so critical to the success of the churn modelling project, it is important to identify the right people. To identify them, you can ask: who cares about this? Who is responsible for reducing churn? Who will take action once the model identifies a high-risk customer? Bring these stakeholders in early. They need to trust the results, or else they might ignore them. Their feedback can often provide ideas for feature engineering, improve data quality, and make the model actionable for the business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c4aca",
   "metadata": {},
   "source": [
    "## Define churn\n",
    "\n",
    "The next step is to define churn, which your stakeholders can help do. This tends to be segmented by the business model. Note that within a company, it is possible to have multiple business models for different revenue streams, so your definition of churn may need to vary by product or service offering!\n",
    "\n",
    "For subscription-based business models, this is typically whether a customer renews their subscription. In our B2B example, customers typically have annual subscriptions, so our churn definition uses whether they renew their contract. Revenue could also be a factor in this definition, where any downsell might also be considered churn.\n",
    "\n",
    "In retail-like business models, where customers make individual purchases (not limited to retail businesses, this applies to other industries too!), this is typically related to whether a customer makes a purchase in some window of time. In our B2C example, churn is defined as a customer who does not make any purchases in the next 3 months. This could be the next 6 months, or the next 30 days, etc. It also could use a revenue threshold, where a customer purchases at least $50 worth of products or services. Ultimately your stakeholders will have the best idea on what definition will be most valuable to the business, which is why it’s important that they sign off on this definition.\n",
    "\n",
    "## Population\n",
    "\n",
    "We also need to define our population, which will impact who we end up training and making predictions on. Determining the boundaries of this population requires understanding the business goals and listening to stakeholders. There may be a specific group of customers that the business cares about retaining, such as mid-sized and large companies. Maybe there is one product that is particularly important. Or there could be no differentiation, and the business wants to predict on all customers. Ultimately, your stakeholders will make this decision, which is why it’s important to discuss this with them.\n",
    "\n",
    "In the B2C example, the focus is on retaining customers who had placed an order in the previous 3 months. This was a commonly used definition for various metrics throughout the business. Using a common definition pays dividends down the churn-modelling road because the model will align with existing processes and analyses. \n",
    "\n",
    "In the B2B example, the population of interest started as customers with a managed cloud subscription. We used this restriction because we had more data available for those customers. Over time, this definition was expanded to include all other customers. This approach allowed us to solve for the easier population first and prove value quickly. Then we could address the population which is harder to predict.\n",
    "\n",
    "## Prediction point\n",
    "\n",
    "Finally, we need to define our prediction point. This is the time at which we will make predictions about churn. If you plan to operationalize your model (make predictions on new customers), it is important this aligns to the time at which you make predictions in production. Remember that the end goal is to prevent churn, so these prediction points need to be early enough for your stakeholders to intervene and prevent churn. These prediction points should also be spaced out far enough that there is a meaningful chance for churn risk to change. If your customers typically make one purchase a week, then making a new prediction every day is unlikely to add much value beyond a model which predicts once a week. The simpler your model is, the easier it will be to build and consume!\n",
    "\n",
    "In the B2C example, the model is used to make predictions every month. The prediction point is the first day of each month and on that date the model predicts the probability that each customer in the population of interest (those who had placed an order in the preceding three months) will not place an order in the following three months, and therefore will churn. \n",
    "\n",
    "In the B2B example, the prediction point is every four weeks, up to 36 weeks prior to the renewal date. This gives one prediction every month for the 9 months before renewal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af18a8",
   "metadata": {},
   "source": [
    "## Model consumption strategy\n",
    "\n",
    "Before diving into the data, it is important to have line of sight into how the business will use the churn model. This will impact modeling choices, such as how strictly we need to follow the prediction point and whether we can include features which would be difficult to use in production.\n",
    "\n",
    "One method of consumption, and a good first objective, is to surface insights in order to reduce churn across the entire customer base. As with most data science projects, it makes sense to begin with exploration. Often a thorough understanding of the problem immediately surfaces potential solutions. A good model will present insights which might, for example, uncover regular churn patterns. Presented to relevant stakeholders, these insights may lead to suggested changes to the product in order to divert customers from those patterns. In this way, model insights can be useful to understanding and addressing churn at an aggregate level. This approach is easier and faster to implement, but likely will have more limited ROI, as it does not provide individual churn predictions for each customer.\n",
    "\n",
    "Second is operationalizing the model to make new predictions in production. This gives each customer their own churn risk and allows end users to prioritize interventions for those which are more likely to churn. For this to be actionable, concrete and cost-effective churn prevention actions are needed. Can we add customer support to the account? Can we offer a promotion? This is why talking to stakeholders at the beginning is important. It teaches us what interventions are possible to prevent churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a790c875",
   "metadata": {},
   "source": [
    "# Data Management\n",
    "\n",
    "With a firm understanding of the problem, we can begin building our training dataset. The first step is to set our prediction point and sampling strategy.\n",
    "\n",
    "## Prediction point and sampling\n",
    "\n",
    "The most common mistake at this stage is to accidentally train the model on data from after the prediction point. This leads to look-ahead bias. A model trained on data from after the prediction point will have lower accuracy in production than it did in validation, because it no longer has access to data from the future (relative to the prediction point). This is why the first step is to create the relevant prediction points for each customer. For example, the B2B example uses a prediction point of every 4 weeks leading up to the renewal date, up to 36 weeks (9 months) prior to the renewal date. The SQL code below shows an example of how you can create each row in the dataset using this framing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f452fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show code\"\n",
    "#| output: false\n",
    "\n",
    "with weeks as (\n",
    "    select \n",
    "        row_number() over (order by seq4()) * 4 as n\n",
    "    from table(generator(rowcount => 9))\n",
    ")\n",
    "select\n",
    "    r.opportunity_id,\n",
    "    r.renewal_week,\n",
    "    dateadd('week', -weeks.n, r.renewal_week) as pred_point,\n",
    "    weeks.n as weeks_to_renewal\n",
    "from renewals as r\n",
    "cross join weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f5a72",
   "metadata": {},
   "source": [
    "If the dataset is large enough, the training dataset can be reduced to one row per customer. This is the recommended approach, as it will make partitioning easier and ensure each customer is equally weighted in the dataset. In this case, we randomly choose a valid prediction point for each customer. Using the B2B example, we would randomly choose one of the 9 months for each customer.\n",
    "\n",
    "In the B2C example, the prediction point is the start of every month. We chose to keep multiple rows per customer, as the dataset was not large enough to develop confident models without them. When using multiple rows per customer, it is important to either use grouped partitioning (grouping on customers such that all rows from one customer are in the same partition) or Out-of-Time Validation. This prevents leakage across the partitions, where a model can learn a specific customer’s behavior.\n",
    "\n",
    "## Target creation\n",
    "\n",
    "Now we can pull in our definition of churn to create the target. Remember to use the definition relative to the prediction point. In the B2C example, the target is whether the customer made any purchases in the next quarter.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa5247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show code\"\n",
    "#| output: false\n",
    "\n",
    "with customers as (\n",
    "    select \n",
    "        customer_id,\n",
    "        min(event_date) as first_purchase\n",
    "    from events\n",
    "    group by 1\n",
    "),\n",
    "customer_months as (\n",
    "    select\n",
    "        c.customer_id,\n",
    "        dc.date_actual as month_start\n",
    "    from customers as c\n",
    "    cross join daily_calendar as dc\n",
    "    where dc.date_actual = dc.first_day_of_month\n",
    "        and dc.date_actual > c.first_purchase\n",
    "),\n",
    "customer_monthly_purchases as (\n",
    "    select\n",
    "        c.customer_id,\n",
    "        c.month_start,\n",
    "        count(e.id) as monthly_number_of_purchases\n",
    "    from customer_months as c\n",
    "    left join events as e on c.customer_id = e.customer_id\n",
    "        and c.month_start = date_trunc('month', e.event_date)::date\n",
    "    where c.month_start < current_date - interval '3 months'\n",
    "    group by 1, 2 \n",
    "),\n",
    "base_table as (\n",
    "    select \n",
    "        customer_id, \n",
    "        month_start as pred_point,\n",
    "        sum(monthly_number_of_purchases) over (\n",
    "            partition by customer_id \n",
    "            order by month_start \n",
    "            rows between 3 preceding and 1 preceding\n",
    "        ) as number_of_purchases_last_3_months,\n",
    "        sum(monthly_number_of_purchases) over (\n",
    "            partition by customer_id \n",
    "            order by month_start \n",
    "            rows between current row and 2 following\n",
    "        ) as number_of_purchases_next_3_months,\n",
    "    (number_of_purchases_next_3_months = 0)::int as churn\n",
    "    from customer_monthly_purchases\n",
    ")\n",
    "select customer_id, pred_point, churn\n",
    "from base_table\n",
    "where number_of_purchases_last_3_months > 0\n",
    "limit 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc03e4fa",
   "metadata": {},
   "source": [
    "![](img/max_sql_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40f0d8",
   "metadata": {},
   "source": [
    "This code generates the primary dataset with CHURN=1 if the customer did not place an order in the upcoming three months, and 0 if they did. Including PREDICTION_POINT in this primary table is important because often the training dataset will be comprised of multiple prediction points. This is useful both to increase the size of the training dataset, as well as to help the model account for seasonality. DataRobot feature engineering will also rely on the PREDICTION_POINT field to avoid look-ahead bias.\n",
    "\n",
    "The B2B model was set up to predict whether a customer would sign a renewal on their renewal date. Again, creating prediction points was necessary to avoid look-ahead bias just like in the B2C example. In this case, though, predictions would be made more frequently and always in reference to that renewal date, e.g. 4 weeks from renewal or 32 weeks out. This way the model could be trained on how different features impact churn probability at different times in the customer lifecycle. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfd3170-888f-4c3c-8a38-731181fdf69c",
   "metadata": {},
   "source": [
    "## Data sources\n",
    "\n",
    "It is not always obvious what data will be predictive of churn, so exploring multiple datasets is worthwhile. Data on product/services consumption are important. Some other datasets to consider are purchase history, customer demographic data, customer surveys, and interactions with customer support. In the B2C example, we used data on customer reviews as well as refunds issued. \n",
    "\n",
    "One way to uncover valuable insights is to include data on actions controllable by the business. If a promotion or a specific marketing campaign turns out to be predictive of churn or retention, that is a quick action item to share with stakeholders. Just ensure these actions were taken before the prediction point, rather than in response to a perceived churn risk. \n",
    "\n",
    "Listen to your stakeholders about their beliefs on what drives churn or retention and include that data when it is available. This can go a long way towards building their trust in the model. If your model validates their beliefs, it shows evidence that it is learning relevant behavior. On the contrary, if the model refutes one of their beliefs, this can spur a conversation about it. There might be bad data in your dataset, or maybe the feature you created does not accurately represent what they think is a driver. It could also be proof that their belief is wrong, which can foster a deeper understanding of churn risk at the company. These discussions and further data investigation are the key to finding out why.\n",
    "\n",
    "At the end of the day, start with whatever data is easily accessible and build models with that. Showing value to the business quickly is more important than exploring every dataset possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bceabe",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Merging all of your disparate data into one table may sound daunting. DataRobot’s automated feature engineering can help in a number of ways. DataRobot feature engineering accelerates data preparation for churn modelling by joining data from disparate datasets, automatically generating a wide variety of features across these datasets, and removing features that have little/no relation to churn. Crucially, it also helps avoid the aforementioned look-ahead bias. DataRobot makes use of time-aware feature engineering to ensure we avoid this. \n",
    "\n",
    "If you prefer to build the dataset outside of DataRobot, make sure your joins are aware of the prediction point, not just the customer ID. In the B2B example, we made heavy use of window functions to create features over a specific period of time. For example, we can join a usage table once but create multiple feature derivation windows, such as number of projects created in the last 4 weeks, last 12 weeks, etc. The SQL below demonstrates how to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e33f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Show code\"\n",
    "#| output: false\n",
    "\n",
    "with weekly_usage_data as (\n",
    "       select\n",
    "              a.account_id,\n",
    "              date_trunc('week', c.date_actual)::date as week_start,\n",
    "              sum(u.projects_created) as projects_created\n",
    "       from accounts as a\n",
    "       inner join daily_calendar as c on a.customer_since_date <= c.date_actual\n",
    "              and current_date >= c.date_actual\n",
    "       left join usage_data as u on a.account_id = u.account_id\n",
    "              and c.date_actual = u.activity_date\n",
    "       group by 1, 2\n",
    ")\n",
    "select\n",
    "       account_id,\n",
    "       week_start,\n",
    "       sum(projects_created) over (partition by account_id\n",
    "                                   order by week_start\n",
    "                                   rows between 12 preceding and 1 preceding) as projects_created_last_12_weeks,\n",
    "       sum(projects_created) over (partition by account_id\n",
    "                                   order by week_start\n",
    "                                   rows between 4 preceding and 1 preceding) as projects_created_last_4_weeks\n",
    "from weekly_usage_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20256fa4",
   "metadata": {},
   "source": [
    "![](img/safer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e487e4",
   "metadata": {},
   "source": [
    "Ultimately you can be as creative as you want. Just make sure your features are interpretable to the business. They will have ownership of making decisions from what the model recommends, so it is important that they understand how the model makes its predictions.\n",
    "\n",
    "With our problem well-framed and our dataset created, we are in good shape to begin modelling. Look for Part 2 in this 3 part series for a discussion of model training and evaluation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
