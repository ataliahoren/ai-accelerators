{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><H1>End to End DataRobot AutoML workflow with Amazon S3</H1></center>\n",
    "\n",
    "<table border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n",
    "<td><img src=\"https://www.datarobot.com/wp-content/uploads/2021/08/DataRobot-logo-color.svg\" height=200px width=200px>\n",
    "</td>\n",
    "<td><font size=10> + </font> </td>\n",
    "<td> <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Amazon-S3-Logo.svg/1712px-Amazon-S3-Logo.svg.png\" height=100px width=100px> </td>\n",
    "\n",
    "Author: Biju Krishnan\n",
    "\n",
    "[API reference documentation](https://docs.datarobot.com/en/docs/api/reference/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>\n",
    "This example notebook outlines the following tasks: <p>\n",
    "<ol>\n",
    "<li> Read PARQUET files from an Amazon S3 bucket into a pandas dataframe using AWS Wrangler Python library </li>\n",
    "<li> Upload a dataset in a dataframe to DataRobot's AI Catalog </li>\n",
    "<li> Initiate a DataRobot AutoML project with the dataset</li>\n",
    "<li> Deploy the top performing model to a DataRobot prediction server. </li>\n",
    "<li> Make batch predictions with a test dataset. </li>\n",
    "</ol>\n",
    "<p>\n",
    "The files stored in S3 used for training can be in any format supported by the AWS Wrangler Python library. For batch predictions, DataRobot supports Parquet and CSV.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datarobot as dr \n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import boto3\n",
    "import awswrangler as wr # This notebooks uses AWS Wrangler because its easy to read multiple files from the S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bind variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind variables\n",
    "# These variables can aso be fetched from a secret store or config files\n",
    "\n",
    "DATAROBOT_ENDPOINT=\"https://app.eu.datarobot.com/api/v2\"\n",
    "# The URL may vary depending on your hosting preference, the above example is for DataRobot EU Managed AI Cloud\n",
    "\n",
    "DATAROBOT_API_TOKEN=\"<INSERT YOUR DataRobot API Token>\"\n",
    "# The API Token can be found by click the avatar icon and then </> Developer Tools\n",
    "\n",
    "client =dr.Client(\n",
    "    token=DATAROBOT_API_TOKEN, \n",
    "    endpoint=DATAROBOT_ENDPOINT,\n",
    "    user_agent_suffix='AIA-E2E-AWS-14' #Optional but helps DataRobot improve this workflow\n",
    ")\n",
    "\n",
    "dr.client._global_client = client\n",
    "\n",
    "AWS_KEY = '<INSERT YOUR AWS ACCESS KEY>' # Enter your AWS Key ID\n",
    "AWS_SECRET = '<INSERT YOUR AWS SECRETS>' # Enter your AWS Secret  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to DataRobot\n",
    "\n",
    "You can read more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.Client(\n",
    "token=DATAROBOT_API_TOKEN,\n",
    "endpoint=DATAROBOT_ENDPOINT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a BOTO3 connection for connection to AWS \n",
    "# This session will be used in the next cell to read files from S3\n",
    "\n",
    "my_session = boto3.Session(\n",
    "    aws_access_key_id=AWS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET,\n",
    "    # aws_session_token = <Optional>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "<font>\n",
    "<p>\n",
    "For illustration purposes, the training dataset containing patient visits to a hospital is stored in an S3 bucket named e2eaccelerator09122022 under the path <code>s3://e2eaccelerator09122022/training/input/</code> .\n",
    "<pre><code><font color=grey size=1>\n",
    "aws s3 ls s3://e2eaccelerator09122022/training/input/\n",
    "2022-12-09 09:55:47          0\n",
    "2022-12-09 09:56:15     267017 10k_diabetes.parquet\n",
    "</font></code></pre>\n",
    "<p>\n",
    "The input folder contains only one file in this scenario, however the code will also work in case of multiple files.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet files from an S3 bucket into a pandas dataframe using AWS Wrangler\n",
    "\n",
    "s3_training_input = \"s3://e2eaccelerator09122022/training/input/\"\n",
    "df = wr.s3.read_parquet(path=s3_training_input,dataset=True,boto3_session=my_session) \n",
    "# Specifying dataset=True allows reading multiple files\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset\n",
    "\n",
    "Create a dataset in the AI Catalog to use it for project creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datarobot_dataset = dr.Dataset.create_from_in_memory_data(data_frame=df,fname=\"10K diabetes E2E accelerator\")\n",
    "datarobot_dataset.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project and initiate Autopilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will take several minutes to complete execution\n",
    "# Creates an AutoML project named \"E2E Demo Amazon S3\" with \"readmitted\" as the target column\n",
    "# Quick mode is the designated training mode in this example, however other modes are also available\n",
    "\n",
    "\n",
    "EXISTING_PROJECT_ID = None # If you've already created a project, replace None with the ID here\n",
    "\n",
    "if EXISTING_PROJECT_ID is None:\n",
    "    # Create project and pass in data\n",
    "    project = dr.Project.create_from_dataset(datarobot_dataset.id,\n",
    "                                project_name = 'E2E Demo Amazon S3')\n",
    "\n",
    "    # Set the project target to the appropriate feature. Use the LogLoss metric to measure performance\n",
    "    project.analyze_and_model(target='readmitted',\n",
    "                       mode=dr.AUTOPILOT_MODE.QUICK, \n",
    "                       worker_count='-1')\n",
    "else:\n",
    "    # Fetch the existing project\n",
    "    project = dr.Project.get(EXISTING_PROJECT_ID)\n",
    "\n",
    "project.wait_for_autopilot(check_interval=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the AutoML project is complete, select the top-performing model on the Leaderboard based on the chosen metric for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_by_metric(models, test_set, metric):\n",
    "    models_with_score = [model for model in models if\n",
    "                         model.metrics[metric][test_set] is not None]\n",
    "    \n",
    "    return sorted(models_with_score,\n",
    "                  key=lambda model: model.metrics[metric][test_set])\n",
    "\n",
    "models = project.get_models()\n",
    "\n",
    "metric = project.metric\n",
    "\n",
    "# Get the top-performing model\n",
    "model_top = sorted_by_metric(models, 'crossValidation', metric)[0]\n",
    "\n",
    "print('''The top performing model is {model} using metric, {metric}'''.format(model = str(model_top), metric = metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy a model\n",
    "\n",
    "Note that steps in the following sections require DataRobot MLOps licensed features. Contact your DataRobot account representatives if you are missing some licensed MLOps features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prediction server\n",
    "prediction_server = dr.PredictionServer.list()[0]\n",
    "\n",
    "# Create a deployment\n",
    "deployment = dr.Deployment.create_from_learning_model(\n",
    "    model_top.id, label='E2E Amazon S3 Test', description='Model trained on 10k diabetes dataset',\n",
    "    default_prediction_server_id=prediction_server.id)\n",
    "deployment.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions\n",
    "\n",
    "<font family=verdana>\n",
    "DataRobot's batch predictions API is capable of directly reading and writing to Amazon S3 storage. \n",
    "<p>\n",
    "<i>Note: Parquet support for batch predictions is still in preview mode. Contact your DataRobot representative to enable the feature flags for trial.</i>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run a batch prediction job you need to store the AWS Credentials in the DataRobot credentials manager\n",
    "# The AWS key and secret should be unique\n",
    "# If they are already stored in the Credentials manager this code will throw an error\n",
    "\n",
    "DR_CREDENTIAL_NAME = \"AWS S3 Credentials\" # Choose a name as per your convenience\n",
    "for cred in dr.Credential.list():\n",
    "    if cred.name == DR_CREDENTIAL_NAME:\n",
    "        cred_flag = False\n",
    "        credential_id = cred.credential_id\n",
    "        break\n",
    "    else:\n",
    "        cred_flag = True\n",
    "\n",
    "if cred_flag:\n",
    "    credential = dr.Credential.create_s3(\n",
    "    name=DR_CREDENTIAL_NAME,\n",
    "    aws_access_key_id = AWS_KEY,\n",
    "    aws_secret_access_key= AWS_SECRET,\n",
    "    #aws_session_token= <Optional>\n",
    "    )\n",
    "    credential_id = credential.credential_id      \n",
    "\n",
    "print(credential_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch predictions snippet\n",
    "\n",
    "The snippet below provides sample code to demonstratehow to make batch predictions to and from Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.BatchPredictionJob._s3_settings = dr.BatchPredictionJob._s3_settings.allow_extra(\"*\")\n",
    "\n",
    "# Use the manipulated batch job class to score:\n",
    "job = dr.BatchPredictionJob.score(\n",
    "    deployment=deployment.id,\n",
    "    intake_settings={\n",
    "        'type': 's3',\n",
    "        'credential_id': credential_id,\n",
    "        'format': 'csv', # Can also be Parquet\n",
    "        \"url\": \"s3://e2eaccelerator09122022/predictions/input/10k_diabetes_test.csv\", ## This can be a path or a file depending on the format chosen\n",
    "    },\n",
    "    output_settings={\n",
    "        'type': 's3',\n",
    "        'credential_id': credential_id,\n",
    "        'format': 'parquet', # Can also be CSV\n",
    "        'url': 's3://e2eaccelerator09122022/predictions/output/10k_diabetes_test.parquet', ## This should point to a file not a path    \n",
    "    },\n",
    ")\n",
    "\n",
    "job.wait_for_completion()\n",
    "job.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font family=verdana>\n",
    "<p>\n",
    "The output of the batch predictions is thus available under the path s3://e2eaccelerator09122022/predictions/output/\n",
    "<pre><code><font color=grey size=1>\n",
    "aws s3 ls s3://e2eaccelerator09122022/predictions/output/\n",
    "2022-12-09 11:35:32          0\n",
    "2022-12-09 14:09:28      21244 10k_diabetes_test.parquet\n",
    "</font></code></pre>\n",
    "</font>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
