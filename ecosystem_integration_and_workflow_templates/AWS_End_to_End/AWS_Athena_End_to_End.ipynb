{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><H1>DataRobot AutoML end-to-end with Amazon Athena</H1></center>\n",
    "\n",
    "<table border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n",
    "<td><img src=\"https://www.datarobot.com/wp-content/uploads/2021/08/DataRobot-logo-color.svg\" height=200px width=200px>\n",
    "</td>\n",
    "<td><font size=10> + </font> </td>\n",
    "<td> <img src=\"https://vectorwiki.com/images/1BalA__aws-athena.svg\" height=100px width=100px> </td>\n",
    "\n",
    "Author: Biju Krishnan\n",
    "\n",
    "[API reference documentation](https://docs.datarobot.com/en/docs/api/reference/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>\n",
    "This example notebook outlines the following tasks: <p>\n",
    "<ol>\n",
    "<li> Read in an Amazon Athena table and upload it to DataRobot's AI Catalog </li>\n",
    "<li> Create a project with the dataset</li>\n",
    "<li> Deploy the top performing model to a DataRobot prediction server </li>\n",
    "<li> Make batch predictions with a test dataset </li>\n",
    "</ol>\n",
    "<p>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datarobot as dr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bind variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables can aso be fetched from a secret store or config files\n",
    "DATAROBOT_ENDPOINT=\"https://app.eu.datarobot.com/api/v2\"\n",
    "# The URL may vary depending on your hosting preference, the above example is for DataRobot EU Managed AI Cloud\n",
    "\n",
    "DATAROBOT_API_TOKEN=\"<INSERT YOUR DataRobot API Token>\"\n",
    "# The API Token can be found by click the avatar icon and then </> Developer Tools\n",
    "\n",
    "client =dr.Client(\n",
    "    token=DATAROBOT_API_TOKEN, \n",
    "    endpoint=DATAROBOT_ENDPOINT,\n",
    "    user_agent_suffix='AIA-E2E-AWS-16' #Optional but helps DataRobot improve this workflow\n",
    ")\n",
    "\n",
    "dr.client._global_client = client\n",
    "\n",
    "AWS_KEY = '<INSERT YOUR AWS ACCESS KEY>' # Enter your AWS Key ID\n",
    "AWS_SECRET = '<INSERT YOUR AWS SECRETS>' # Enter your AWS Secret  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to DataRobot\n",
    "\n",
    "You can read more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.Client(\n",
    "token=DATAROBOT_API_TOKEN,\n",
    "endpoint=DATAROBOT_ENDPOINT,\n",
    "ssl_verify= False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line reads the driver object needed for creating a datastore\n",
    "athena_driver = [drv for drv in dr.DataDriver.list() if drv.canonical_name == 'AWS Athena (v5)'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "### Create a data connection\n",
    "\n",
    "Use the cell below to define the parameters required to make a connector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_s3_bucket = \"e2eaccelerator09122022\" # Specifythe name of the bucket followed by any prefix, later you format it as an S3 URI\n",
    "\n",
    "jdbc_url = \"jdbc:awsathena://athena.eu-west-1.amazonaws.com;AwsRegion=eu-west-1;S3OutputLocation=s3://{}/\".format(athena_s3_bucket)\n",
    "\n",
    "query = 'SELECT * FROM \"new_york_taxi\".\"input\" limit 10000;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data connection (AKA a datastore)\n",
    "\n",
    "DR_DATASTORE_NAME = \"ATHENA Data Connection\" # This name can be altered\n",
    "\n",
    "# Checking if datastore already exists\n",
    "for dstore in dr.DataStore.list():\n",
    "    if dstore.canonical_name==DR_DATASTORE_NAME:\n",
    "        datastore_flag = False\n",
    "        datastore = dstore\n",
    "        break\n",
    "    else:\n",
    "        datastore_flag = True\n",
    "\n",
    "if datastore_flag:\n",
    "    datastore = dr.DataStore.create(data_store_type='jdbc', \n",
    "                                canonical_name='ATHENA Data Connection', # This name can be replaced\n",
    "                                driver_id=athena_driver.id, \n",
    "                                jdbc_url=jdbc_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data connection based on a query\n",
    "# The Athena JDBC driver only supports query-based ingestion\n",
    "\n",
    "params = dr.DataSourceParameters(data_store_id=datastore.id, \n",
    "                                 query=query)\n",
    "\n",
    "DR_DATASOURCE_NAME = \"ATHENA Data Source\" # This name can be altered\n",
    "\n",
    "for dsource in dr.DataSource.list():\n",
    "    if dsource.canonical_name==DR_DATASOURCE_NAME:\n",
    "        datasource_flag = False\n",
    "        datasource = dsource\n",
    "        break\n",
    "    else:\n",
    "        datasource_flag = True\n",
    "\n",
    "if datasource_flag:\n",
    "    datasource = dr.DataSource.create(data_source_type='jdbc', \n",
    "                                canonical_name='ATHENA Data Source', # This name can be altered\n",
    "                                params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code snippet creates a snapshot of the Athena table and stores it in the AI Catalog\n",
    "\n",
    "datarobot_dataset = dr.Dataset.create_from_data_source(data_source_id=datasource.id,username=AWS_KEY,password=AWS_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a project and initiate Autopilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will take several minutes to complete execution\n",
    "# An AutoML project named \"E2E Demo Amazon Athena\" is created with \"tip_amount\" as the target column\n",
    "# Quick mode is designated, however other modes are also available\n",
    "\n",
    "\n",
    "EXISTING_PROJECT_ID = None # If you've already created a project, replace None with the ID here\n",
    "\n",
    "if EXISTING_PROJECT_ID is None:\n",
    "    # Create project and pass in data\n",
    "    project = dr.Project.create_from_dataset(datarobot_dataset.id,\n",
    "                                project_name = 'E2E Demo Amazon Athena')\n",
    "\n",
    "    # Set the project target to the appropriate feature. Use the LogLoss metric to measure performance\n",
    "    project.analyze_and_model(target='tip_amount',\n",
    "                       mode=dr.AUTOPILOT_MODE.QUICK, \n",
    "                       worker_count='-1')\n",
    "else:\n",
    "    # Fetch the existing project\n",
    "    project = dr.Project.get(EXISTING_PROJECT_ID)\n",
    "\n",
    "project.wait_for_autopilot(check_interval=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the top-performing model\n",
    "\n",
    "Once the AutoML project is complete, select the top-performing model on the Leaderboard based on the chosen metric for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_by_metric(models, test_set, metric):\n",
    "    models_with_score = [model for model in models if\n",
    "                         model.metrics[metric][test_set] is not None]\n",
    "    \n",
    "    return sorted(models_with_score,\n",
    "                  key=lambda model: model.metrics[metric][test_set])\n",
    "\n",
    "models = project.get_models()\n",
    "\n",
    "metric = project.metric\n",
    "\n",
    "# Get the top-performing model\n",
    "model_top = sorted_by_metric(models, 'crossValidation', metric)[0]\n",
    "\n",
    "print('''The top performing model is {model} using metric, {metric}'''.format(model = str(model_top), metric = metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy a model\n",
    "\n",
    "Note that steps in the following sections require DataRobot MLOps licensed features. Contact your DataRobot account representatives if you are missing some licensed MLOps features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prediction server\n",
    "prediction_server = dr.PredictionServer.list()[0]\n",
    "\n",
    "# Create a deployment\n",
    "deployment = dr.Deployment.create_from_learning_model(\n",
    "    model_top.id, label='E2E Amazon Athena Test', description='Model trained on New York Taxi trips dataset',\n",
    "    default_prediction_server_id=prediction_server.id)\n",
    "deployment.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions\n",
    "\n",
    "DataRobot's batch predictions API is capable of directly reading and writing to Amazon S3 storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run a batch prediction job you need to store the Amazon Athena Credentials in the DataRobot credentials manager\n",
    "\n",
    "DR_CREDENTIAL_NAME = \"Amazon Athena Credentials\" # Choose a name\n",
    "for cred in dr.Credential.list():\n",
    "    if cred.name == DR_CREDENTIAL_NAME:\n",
    "        cred_flag = False\n",
    "        athena_credential_id = cred.credential_id\n",
    "        break\n",
    "    else:\n",
    "        cred_flag = True\n",
    "\n",
    "# Create credentials in DataRobot credential store if they do not exist\n",
    "if cred_flag:\n",
    "    credential = dr.Credential.create_basic(\n",
    "        name=DR_CREDENTIAL_NAME, # The username and password is the AWS KEY and SECRET respectively\n",
    "        user = AWS_KEY,\n",
    "        password= AWS_SECRET,\n",
    "        )\n",
    "    athena_credential_id = credential.credential_id      \n",
    "\n",
    "print(athena_credential_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DR_CREDENTIAL_NAME = \"AWS S3 Credentials\" # Choose a name as per your convenience\n",
    "for cred in dr.Credential.list():\n",
    "    if cred.name == DR_CREDENTIAL_NAME:\n",
    "        cred_flag = False\n",
    "        s3_credential_id = cred.credential_id\n",
    "        break\n",
    "    else:\n",
    "        cred_flag = True\n",
    "\n",
    "# Create credentials in DataRobot credential store if it does not exist\n",
    "if cred_flag:\n",
    "    credential = dr.Credential.create_s3(\n",
    "    name=DR_CREDENTIAL_NAME,\n",
    "    aws_access_key_id = AWS_KEY,\n",
    "    aws_secret_access_key= AWS_SECRET,\n",
    "    #aws_session_token= <Optional>\n",
    "    )\n",
    "    s3_credential_id = credential.credential_id      \n",
    "\n",
    "print(s3_credential_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch predictions snippet\n",
    "\n",
    "The snippet below provides sample code to demonstratehow to make batch predictions to and from Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example scores the training data but there needs to be an Athena table with test data.\n",
    "\n",
    "job = dr.BatchPredictionJob.score(\n",
    "    deployment=deployment.id,    \n",
    "    intake_settings = {\n",
    "    'type': 'jdbc',\n",
    "    'query': \"select * from new_york_taxi.input limit 1000\",  # This has to be a query, since the JDBC driver does not seem to understand table schema structure\n",
    "    'data_store_id': datastore.id, # The ID of the datastore you want\n",
    "    'credential_id': athena_credential_id # The credentialid of the credentials stored in your credentials manager\n",
    "    },\n",
    "    output_settings={\n",
    "        'type': 's3',\n",
    "        'url': \"s3://e2eaccelerator09122022/predictions/output/new_york_taxi_predictions.csv\", # Note this has to be a filename and not just a bucket name\n",
    "        \"credential_id\": s3_credential_id\n",
    "    }\n",
    ")\n",
    "job.wait_for_completion()\n",
    "job.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font family=verdana>\n",
    "<p>\n",
    "The output of the batch predictions is available under the path s3://e2eaccelerator09122022/predictions/output/\n",
    "<pre><code><font color=grey size=1>\n",
    "aws s3 ls s3://e2eaccelerator09122022/predictions/output/new_york_taxi_predictions.csv\n",
    "2022-12-12 17:47:49      22725 new_york_taxi_predictions.csv\n",
    "</font></code></pre>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
