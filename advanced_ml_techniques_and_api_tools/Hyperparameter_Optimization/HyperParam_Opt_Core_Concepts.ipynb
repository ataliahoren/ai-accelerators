{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e7a029c-9187-4444-b76c-4a3109f934ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hyperparameter optimization workflow\n",
    "\n",
    "Author: Bogdan Tsal-Tsalko\n",
    "\n",
    "Version Date: 01/22/2023\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook and the functions in 'helpers.py' provide a repeatable framework for executing hyperparameter search via DataRobot's Python client.\n",
    "\n",
    "### Hyperparameters for DataRobot modeling\n",
    "\n",
    "In machine learning, hyperparameter tuning is the act of adjusting the \"settings\" (referred to as hyperparameters) in a machine learning algorithm, whether that's the learning rate for an XGBoost model or the activation function in a neural network. Many methods for doing this exist, with the simplest being a brute force search over every feasible combination. While this requires little effort, it's extremely time-consuming as each combination requires fitting the machine learning algorithm. To this end, practitioners strive to find more efficient ways to search for the best combination of hyperparameters to use in a given prediction problem. DataRobot employs a proprietary version of [pattern search](https://app.datarobot.com/docs/modeling/analyze-models/evaluate/adv-tuning.html#set-the-search-type) for optimization not only for the machine learning algorithm's specific hyperparameters, but also the *respective data preprocessing needed to fit the algorithm*, with the goal of quickly producing high-performance models tailored to your dataset.  \n",
    "\n",
    "While the approach used at DataRobot is sufficient in most cases, you may want to build upon DataRobot's Autopilot modeling process by custom tuning methods. In this AI Accelerator, you will familiarize yourself with DataRobot's fine-tuning API calls to control DataRobot's pattern search approach as well as implement a modified brute-force gridsearch for the text and categorical data pipeline and hyperparameters of an XGBoost model. This notebook serves as an introductory learning example that other approaches can be built from.  Bayesian Optimization, for example, leverages a probabilistic model to judiciously sift through the hyperparameter space to converge on an optimal solution, and will be presented next in this accelerator bundle.\n",
    "\n",
    "Note that as a best practice, it is generally best to wait until the model is in a near-finished state before searching for the best hyperparameters to use. Specifically, the following have already been finalized:\n",
    "\n",
    "- Training data (e.g., data sources)\n",
    "- Model validation method (e.g., group cross-validation, random cross-validation, or backtesting. How the problem is framed influences all subsequent steps, as it changes error minimization.)\n",
    "- Feature engineering (particularly, calculations driven by subject matter expertise)\n",
    "- Preprocessing and data transformations (e.g., word or character tokenizers, PCA, embeddings, normalization, etc.)\n",
    "- Algorithm type (e.g. GLM, tree-based, neural net)\n",
    "\n",
    "These decisions typically have a larger impact on model performance compared to adjusting a machine learning algorithm's hyperparameters (especially when using DataRobot, as the hyperparameters chosen automatically are pretty competitive).  \n",
    "\n",
    "### Problem set-up\n",
    "\n",
    "You have settled on a near-final model, and getting that last extra bit of performance matters for your use case. You want to perform a more exhaustive search, or perhaps as an advanced data scientist you have certain parameters you'd like to experiment with. \n",
    "\n",
    "### Advanced tuning and gridsearch approach\n",
    "\n",
    "This notebook helps you learn how to access, understand, and tune blueprints for both preprocessing and model hyperparameters. You'll programmatically work with DataRobot [advanced tuning](https://docs.datarobot.com/en/docs/modeling/analyze-models/evaluate/adv-tuning.html) which you can then adapt to your other projects.\n",
    "\n",
    "You'll learn how to:\n",
    "\n",
    "* Prepare for tuning a model via the DataRobot API\n",
    "    - Load a project and model for tuning\n",
    "    - Set the validation type for minimizing error\n",
    "    - Extracting model metadata\n",
    "    - Get model performance\n",
    "    - Review hyperparameters\n",
    "* Run a single advanced tuning session\n",
    "* Implement your own custom gridsearch for single and multiple models to evaluate\n",
    "\n",
    "    \n",
    "### Prerequsites\n",
    "\n",
    "- A completed DataRobot project (example data can be accessed here: [API](https://datarobot-public-api-client.readthedocs-hosted.com/en/v2.25.1/examples/lending_club/Predicting_Bad_Loans.html) and a [notebook](https://docs.datarobot.com/en/docs/api/guide/common-case/loan-default/loan-default-nb.html)).\n",
    "\n",
    "- Review requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e375a98-dc8a-43d8-a7b9-797477e3374b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb22cb7-abd9-4eb1-96d0-3c5d027a9f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datarobot as dr\n",
    "import graphviz\n",
    "import pandas as pd\n",
    "from datarobot.errors import AsyncProcessUnsuccessfulError, JobNotFinished\n",
    "from datarobot.models.job import Job\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46efb180-8732-4e30-ba3c-a5c7c163fd35",
   "metadata": {},
   "source": [
    "### Connect to DataRobot\n",
    "\n",
    "Read more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d846ce18-de7b-49c6-a993-bc0df0a9de1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datarobot.rest.RESTClientObject at 0x105de18b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c068ddb7-603f-4db4-b1c9-ac700c8c618a",
   "metadata": {},
   "source": [
    "### Optional: create a project\n",
    "\n",
    "Create a project from a URL in DataRobot and follow the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad2cac4-7a37-4aac-98b2-c1d50e94af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://s3.amazonaws.com/datarobot_public_datasets/10K_Lending_Club_Loans.csv'\n",
    "project = dr.Project.create(\n",
    "    url, project_name=\"Loan_default_hyperopt\"\n",
    ")\n",
    "print('Project ID: {}'.format(project.id))\n",
    "project.analyze_and_model(\n",
    "    target='is_bad', \n",
    "    worker_count=-1,\n",
    "    mode=dr.enums.AUTOPILOT_MODE.FULL_AUTO,\n",
    ")\n",
    "project.wait_for_autopilot(verbosity= dr.enums.VERBOSITY_LEVEL.SILENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b294ac47-01e8-4e46-918e-223023e21b7a",
   "metadata": {},
   "source": [
    "## Prepare for tuning a model\n",
    "\n",
    "### Select a model\n",
    "\n",
    "To start tuning, select the model you want to tune from an existing DataRobot project. Your project ID and model ID might be accessed through the API via `dr.Project.list(search_params=({'project_name':'YOUR_SEARCH_KEY'}))`, or simply from the URL of your project.  Alternatively, the helper function below returns a summary of the Leaderboard for a given project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3335c418-2317-4ba0-baab-fa39e9152a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project URL: https://app.datarobot.com/projects/63dd8d10a2633602cfeb80d1/eda\n",
      "Project ID: 63dd8d10a2633602cfeb80d1\n",
      "Unique blueprints tested: 39\n",
      "Feature lists tested: 2\n",
      "Models trained: 65\n",
      "Blueprints in the project repository: 280\n",
      "Feature engineering and preprocessing steps ran:  2455\n",
      "\n",
      "\n",
      "Top models in the leaderboard:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>model</th>\n",
       "      <th>pct</th>\n",
       "      <th>validation_LogLoss</th>\n",
       "      <th>cross_validation_LogLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63dd8e84f8dd7f9f66de9c1c</td>\n",
       "      <td>Light Gradient Boosted Trees Classifier with E...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.36734</td>\n",
       "      <td>0.357414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63dd8e84f8dd7f9f66de9c20</td>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.36780</td>\n",
       "      <td>0.357710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63dd8e84f8dd7f9f66de9c1b</td>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.36571</td>\n",
       "      <td>0.357786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63dd8e84f8dd7f9f66de9c1d</td>\n",
       "      <td>Gradient Boosted Trees Classifier with Early S...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.36608</td>\n",
       "      <td>0.357796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63dd8fe5cbfcbbaaec9043b2</td>\n",
       "      <td>Light Gradient Boosted Trees Classifier with E...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.36827</td>\n",
       "      <td>0.357800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model_id  \\\n",
       "0  63dd8e84f8dd7f9f66de9c1c   \n",
       "1  63dd8e84f8dd7f9f66de9c20   \n",
       "2  63dd8e84f8dd7f9f66de9c1b   \n",
       "3  63dd8e84f8dd7f9f66de9c1d   \n",
       "4  63dd8fe5cbfcbbaaec9043b2   \n",
       "\n",
       "                                               model   pct  \\\n",
       "0  Light Gradient Boosted Trees Classifier with E...  64.0   \n",
       "1  eXtreme Gradient Boosted Trees Classifier with...  64.0   \n",
       "2  eXtreme Gradient Boosted Trees Classifier with...  64.0   \n",
       "3  Gradient Boosted Trees Classifier with Early S...  64.0   \n",
       "4  Light Gradient Boosted Trees Classifier with E...  64.0   \n",
       "\n",
       "   validation_LogLoss  cross_validation_LogLoss  \n",
       "0             0.36734                  0.357414  \n",
       "1             0.36780                  0.357710  \n",
       "2             0.36571                  0.357786  \n",
       "3             0.36608                  0.357796  \n",
       "4             0.36827                  0.357800  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To access an existing project set your project ID below\n",
    "#project = dr.Project.get(\"project_id_see_example_below\") \n",
    "\n",
    "print(\"Project URL: \" + \"https://app.datarobot.com/projects/\" + project.id + \"/eda\")\n",
    "print(\"Project ID: \" + project.id)\n",
    "\n",
    "leaderboard_top = get_top_of_leaderboard(project, metric=\"LogLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3186171-3df3-40bd-b2f9-ee852803a3fd",
   "metadata": {},
   "source": [
    "The example cell below retrieves the XGBoost blueprint with Unsupervised Learning Features, which will be used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f3af9d5-34a1-4ad4-8c85-a9c1ebcda7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_from_search = [bp for bp in project.get_blueprints() if 'Unsupervised Learning Features' in bp.model_type][0].id  #search the blueprint repository\n",
    "\n",
    "model =Job.get(\n",
    "    project_id=project.id, \n",
    "    job_id=project.train(bp_from_search) #train the blueprint or get the model results\n",
    ").get_result_when_complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcf48c9-10bc-4e8b-ac54-ba5aaf601708",
   "metadata": {},
   "source": [
    "Alternative approach to select a model from the top models output.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01ff6ebc-331c-4661-83c7-1b1f00dec9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model('eXtreme Gradient Boosted Trees Classifier with Early Stopping and Unsupervised Learning Features')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = dr.Model.get(\n",
    "    project=project.id,\n",
    "    model_id=leaderboard_top.iloc[2][   # Select a model from the top-performing models in the Leaderboard\n",
    "        \"model_id\"\n",
    "    ],  \n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e2984-4f95-4ac8-b65a-5c32fef303f5",
   "metadata": {},
   "source": [
    "### Inspect a blueprint\n",
    "\n",
    "Before you start tuning, it is conceptually important to reiterate what is contained within a blueprint. In addition to the learning algorithm hyperparmeters, you also have the option to experiment with the hyperparameters of the tasks prior to the learner. \n",
    "\n",
    "For more information please visit the documentation on [blueprints](https://docs.datarobot.com/en/docs/modeling/analyze-models/describe/blueprints.html). \n",
    "\n",
    "You can visualize the processes for your selected model using the cell below to generate a chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258bf5ed-876a-4a7c-97f0-edf9a7e56c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlueprintChart(24 nodes, 31 edges)\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 7.0.6 (20230106.0513)\n",
       " -->\n",
       "<!-- Title: Blueprint Chart Pages: 1 -->\n",
       "<svg width=\"2613pt\" height=\"435pt\"\n",
       " viewBox=\"0.00 0.00 2612.79 435.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 431)\">\n",
       "<title>Blueprint Chart</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-431 2608.79,-431 2608.79,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27.3\" cy=\"-128\" rx=\"27.1\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27.3\" y=\"-124.3\" font-family=\"Times,serif\" font-size=\"14.00\">Data</text>\n",
       "</g>\n",
       "<!-- &#45;3 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>&#45;3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"167.29\" cy=\"-301\" rx=\"61.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"167.29\" y=\"-297.3\" font-family=\"Times,serif\" font-size=\"14.00\">Text Variables</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;&#45;3 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;&#45;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M41.03,-143.97C64.84,-173.81 116.28,-238.31 145.29,-274.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"142.24,-276.46 151.21,-282.1 147.71,-272.1 142.24,-276.46\"/>\n",
       "</g>\n",
       "<!-- &#45;2 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>&#45;2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"167.29\" cy=\"-128\" rx=\"76.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"167.29\" y=\"-124.3\" font-family=\"Times,serif\" font-size=\"14.00\">Numeric Variables</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;&#45;2 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>0&#45;&gt;&#45;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.58,-128C61.84,-128 70.13,-128 78.85,-128\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"78.57,-131.5 88.57,-128 78.57,-124.5 78.57,-131.5\"/>\n",
       "</g>\n",
       "<!-- &#45;1 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>&#45;1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"385.27\" cy=\"-47\" rx=\"85.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"385.27\" y=\"-43.3\" font-family=\"Times,serif\" font-size=\"14.00\">Categorical Variables</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;&#45;1 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>0&#45;&gt;&#45;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M49.75,-117.26C61.56,-111.72 76.61,-105.26 90.6,-101 159.32,-80.09 239.69,-65.99 298.96,-57.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"299.24,-60.96 308.65,-56.09 298.26,-54.03 299.24,-60.96\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"385.27\" cy=\"-409\" rx=\"105.08\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"385.27\" y=\"-405.3\" font-family=\"Times,serif\" font-size=\"14.00\">Converter for Text Mining</text>\n",
       "</g>\n",
       "<!-- &#45;3&#45;&gt;5 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>&#45;3&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M187.43,-318.38C208.6,-336.72 244.45,-365.25 279.98,-382 287.83,-385.7 296.26,-388.95 304.81,-391.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"303.62,-395.08 314.21,-394.7 305.7,-388.39 303.62,-395.08\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"385.27\" cy=\"-355\" rx=\"105.08\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"385.27\" y=\"-351.3\" font-family=\"Times,serif\" font-size=\"14.00\">Converter for Text Mining</text>\n",
       "</g>\n",
       "<!-- &#45;3&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>&#45;3&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M215.42,-312.78C243.96,-319.92 280.91,-329.16 312.97,-337.17\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"312.06,-340.55 322.61,-339.58 313.76,-333.76 312.06,-340.55\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"385.27\" cy=\"-301\" rx=\"105.08\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"385.27\" y=\"-297.3\" font-family=\"Times,serif\" font-size=\"14.00\">Converter for Text Mining</text>\n",
       "</g>\n",
       "<!-- &#45;3&#45;&gt;9 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>&#45;3&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M229.6,-301C241.88,-301 255.15,-301 268.59,-301\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"268.37,-304.5 278.37,-301 268.37,-297.5 268.37,-304.5\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"385.27\" cy=\"-247\" rx=\"105.08\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"385.27\" y=\"-243.3\" font-family=\"Times,serif\" font-size=\"14.00\">Converter for Text Mining</text>\n",
       "</g>\n",
       "<!-- &#45;3&#45;&gt;11 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>&#45;3&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M215.42,-289.22C243.96,-282.08 280.91,-272.84 312.97,-264.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"313.76,-268.24 322.61,-262.42 312.06,-261.45 313.76,-268.24\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>6</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"772.89\" cy=\"-409\" rx=\"246.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"772.89\" y=\"-405.3\" font-family=\"Times,serif\" font-size=\"14.00\">Auto&#45;Tuned Word N&#45;Gram Text Modeler using token occurrences</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M490.63,-409C498.52,-409 506.67,-409 515,-409\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"514.95,-412.5 524.95,-409 514.95,-405.5 514.95,-412.5\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>13</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1202.76\" cy=\"-301\" rx=\"61.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1202.76\" y=\"-297.3\" font-family=\"Times,serif\" font-size=\"14.00\">Bind branches</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;13 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>6&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M950.34,-396.21C973.71,-392.59 997.19,-387.95 1019.22,-382 1071.01,-368.02 1126.88,-341.35 1162.97,-322.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1164.53,-325.57 1171.73,-317.79 1161.25,-319.39 1164.53,-325.57\"/>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>19</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2106.03\" cy=\"-122\" rx=\"369.33\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2106.03\" y=\"-118.3\" font-family=\"Times,serif\" font-size=\"14.00\">eXtreme Gradient Boosted Trees Classifier with Early Stopping and Unsupervised Learning Features</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;19 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>13&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1254.79,-290.86C1398.92,-262.24 1811.01,-180.4 2003.57,-142.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2004.2,-145.59 2013.33,-140.21 2002.84,-138.73 2004.2,-145.59\"/>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>20</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2558\" cy=\"-122\" rx=\"46.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2558\" y=\"-118.3\" font-family=\"Times,serif\" font-size=\"14.00\">Prediction</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;20 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>19&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2475.77,-122C2484.29,-122 2492.34,-122 2499.82,-122\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2499.57,-125.5 2509.57,-122 2499.57,-118.5 2499.57,-125.5\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"772.89\" cy=\"-355\" rx=\"246.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"772.89\" y=\"-351.3\" font-family=\"Times,serif\" font-size=\"14.00\">Auto&#45;Tuned Word N&#45;Gram Text Modeler using token occurrences</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M490.63,-355C498.52,-355 506.67,-355 515,-355\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"514.95,-358.5 524.95,-355 514.95,-351.5 514.95,-358.5\"/>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;13 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M917.99,-340.05C951.26,-336.34 986.5,-332.22 1019.22,-328 1058.21,-322.97 1101.76,-316.56 1136.55,-311.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1136.67,-314.77 1146.02,-309.79 1135.6,-307.85 1136.67,-314.77\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"772.89\" cy=\"-301\" rx=\"246.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"772.89\" y=\"-297.3\" font-family=\"Times,serif\" font-size=\"14.00\">Auto&#45;Tuned Word N&#45;Gram Text Modeler using token occurrences</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M490.63,-301C498.52,-301 506.67,-301 515,-301\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"514.95,-304.5 524.95,-301 514.95,-297.5 514.95,-304.5\"/>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;13 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>10&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1019.37,-301C1059.4,-301 1097.92,-301 1129.16,-301\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1128.82,-304.5 1138.82,-301 1128.82,-297.5 1128.82,-304.5\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"772.89\" cy=\"-247\" rx=\"246.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"772.89\" y=\"-243.3\" font-family=\"Times,serif\" font-size=\"14.00\">Auto&#45;Tuned Word N&#45;Gram Text Modeler using token occurrences</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;12 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>11&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M490.63,-247C498.52,-247 506.67,-247 515,-247\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"514.95,-250.5 524.95,-247 514.95,-243.5 514.95,-250.5\"/>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M917.99,-261.95C951.26,-265.66 986.5,-269.78 1019.22,-274 1058.21,-279.03 1101.76,-285.44 1136.55,-290.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1135.6,-294.15 1146.02,-292.21 1136.67,-287.23 1135.6,-294.15\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"385.27\" cy=\"-155\" rx=\"96.68\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"385.27\" y=\"-151.3\" font-family=\"Times,serif\" font-size=\"14.00\">Missing Values Imputed</text>\n",
       "</g>\n",
       "<!-- &#45;2&#45;&gt;2 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>&#45;2&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M236.03,-136.47C253.94,-138.71 273.58,-141.16 292.53,-143.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"292.04,-147 302.4,-144.77 292.91,-140.05 292.04,-147\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>14</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"385.27\" cy=\"-101\" rx=\"96.68\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"385.27\" y=\"-97.3\" font-family=\"Times,serif\" font-size=\"14.00\">Missing Values Imputed</text>\n",
       "</g>\n",
       "<!-- &#45;2&#45;&gt;14 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>&#45;2&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M236.03,-119.53C253.94,-117.29 273.58,-114.84 292.53,-112.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"292.91,-115.95 302.4,-111.23 292.04,-109 292.91,-115.95\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"772.89\" cy=\"-193\" rx=\"89.08\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"772.89\" y=\"-189.3\" font-family=\"Times,serif\" font-size=\"14.00\">Search for differences</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M471.38,-163.39C533.41,-169.5 617.36,-177.77 680.76,-184.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"680.14,-187.48 690.44,-184.97 680.83,-180.51 680.14,-187.48\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1202.76\" cy=\"-151\" rx=\"61.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1202.76\" y=\"-147.3\" font-family=\"Times,serif\" font-size=\"14.00\">Bind branches</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M482.22,-154.53C647.38,-153.72 981.84,-152.08 1129.22,-151.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1128.86,-154.86 1138.84,-151.31 1128.82,-147.86 1128.86,-154.86\"/>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M853.87,-185.15C933.93,-177.29 1055.88,-165.32 1132.09,-157.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1132.33,-161.33 1141.94,-156.87 1131.65,-154.37 1132.33,-161.33\"/>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;19 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>4&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1264.64,-149.04C1370.05,-145.65 1593.5,-138.46 1783.5,-132.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1783.42,-135.85 1793.3,-132.03 1783.2,-128.85 1783.42,-135.85\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>15</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"772.89\" cy=\"-101\" rx=\"51.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"772.89\" y=\"-97.3\" font-family=\"Times,serif\" font-size=\"14.00\">Standardize</text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;15 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>14&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M482.09,-101C553.02,-101 647.64,-101 709.15,-101\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"709.11,-104.5 719.11,-101 709.11,-97.5 709.11,-104.5\"/>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>17</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1202.76\" cy=\"-97\" rx=\"147.57\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1202.76\" y=\"-93.3\" font-family=\"Times,serif\" font-size=\"14.00\">Partial Principal Components Analysis</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;17 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>15&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M825.08,-100.52C879.09,-100.02 966.94,-99.2 1044.22,-98.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1044.06,-101.97 1054.02,-98.38 1043.99,-94.97 1044.06,-101.97\"/>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>18</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1543.58\" cy=\"-99\" rx=\"83.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1543.58\" y=\"-95.3\" font-family=\"Times,serif\" font-size=\"14.00\">K&#45;Means Clustering</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;18 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>17&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1350.4,-97.87C1383.7,-98.06 1418.2,-98.27 1448.62,-98.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1448.54,-101.94 1458.56,-98.5 1448.58,-94.94 1448.54,-101.94\"/>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;19 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>18&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1626.07,-102.34C1675.25,-104.36 1741.25,-107.07 1808.51,-109.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1808.1,-113.32 1818.24,-110.23 1808.39,-106.32 1808.1,-113.32\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1543.58\" cy=\"-18\" rx=\"157.07\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1543.58\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Ordinal encoding of categorical variables</text>\n",
       "</g>\n",
       "<!-- &#45;1&#45;&gt;1 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>&#45;1&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M443.59,-33.44C468.87,-28.12 499.04,-22.65 526.56,-20 832.07,9.43 1191.64,-0.22 1390.66,-9.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1390.4,-12.92 1400.55,-9.89 1390.73,-5.92 1390.4,-12.92\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>16</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"772.89\" cy=\"-47\" rx=\"77.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"772.89\" y=\"-43.3\" font-family=\"Times,serif\" font-size=\"14.00\">One&#45;Hot Encoding</text>\n",
       "</g>\n",
       "<!-- &#45;1&#45;&gt;16 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>&#45;1&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M470.9,-47C534.06,-47 620.12,-47 684.05,-47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"683.8,-50.5 693.8,-47 683.8,-43.5 683.8,-50.5\"/>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;19 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>1&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1628.48,-33.57C1726.99,-51.85 1890.54,-82.2 1998.07,-102.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1997.31,-105.57 2007.78,-103.95 1998.59,-98.69 1997.31,-105.57\"/>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;17 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>16&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M843.98,-54.42C893.07,-59.69 960.18,-67.03 1019.22,-74 1041.57,-76.64 1065.45,-79.57 1088.32,-82.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1087.6,-85.86 1097.95,-83.63 1088.47,-78.92 1087.6,-85.86\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x1175d3ca0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpChart = model.blueprint.get_chart()\n",
    "print(bpChart)\n",
    "src = graphviz.Source(bpChart.to_graphviz())\n",
    "src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cd445f-997c-42a0-b9e2-28c52dd2cd6f",
   "metadata": {},
   "source": [
    "To further understand what is happening above:\n",
    "\n",
    "- Each of the four text variables in the feature list used by the model is being passed through an Auto-Tuned Word N-Gram Text Modeler, which fits a single-word n-gram model to each text feature in the input dataset, then use the predictions from these models as inputs to an ElasticNet classifier. Further post-processing optimizes the weights of the text-vector via grid search.\n",
    "\n",
    "- Numeric variables go through two paths:\n",
    "    - A greedy search for differences between columns to identify new features.\n",
    "    - Standardizaztion by removing the median and scaling to unit variance or mean absolute deviation. Scaled features are combined with one-hot encoded categoricals, which produces a wide matrix that is fed into PCA, then into k-means to identify clusters of latent features.\n",
    "    \n",
    "- Categoricals go through the above path into PCA, as well as through ordinal encoding in a separate path.\n",
    "\n",
    "Each of these paths are fed in to XGBoost. You can also access documentation around each process using the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79b20945-f710-43bd-9975-0e36ba968b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "references for {}: Truncated SVD tasks\n",
      "0 - sklearn TruncatedSVD: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
      "1 - sklearn decomposition (matrix factorization) user guide: http://scikit-learn.org/stable/modules/decomposition.html#lsa\n"
     ]
    }
   ],
   "source": [
    "bpDoc = model.blueprint.get_documents()[-1]\n",
    "print(\"references for {}:\", format(bpDoc.task))\n",
    "for (i, link) in enumerate(bpDoc.links):\n",
    "    print(\"{} - {}: {}\".format(i, link[\"name\"], link[\"url\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9563dd60-541e-428f-a816-064febb59de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model type: eXtreme Gradient Boosted Trees Classifier with Early Stopping and Unsupervised Learning Features\n",
      "processes:\n",
      "0 - Ordinal encoding of categorical variables\n",
      "1 - Missing Values Imputed\n",
      "2 - Search for differences\n",
      "3 - Converter for Text Mining\n",
      "4 - Auto-Tuned Word N-Gram Text Modeler using token occurrences\n",
      "5 - Standardize\n",
      "6 - One-Hot Encoding\n",
      "7 - Partial Principal Components Analysis\n",
      "8 - K-Means Clustering\n",
      "9 - eXtreme Gradient Boosted Trees Classifier with Early Stopping and Unsupervised Learning Features\n"
     ]
    }
   ],
   "source": [
    "print(\"model type: {}\".format(model.model_type))\n",
    "print(\"processes:\")\n",
    "for (i, p) in enumerate(model.blueprint.processes):\n",
    "    print(\"{} - {}\".format(i, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a889bff7-ae81-46bc-abf4-63b213e4475d",
   "metadata": {},
   "source": [
    "### Extract tunable parameters as a dataframe. \n",
    "\n",
    "The function below returns all hyperparameters as a dataframe. Note- text pipelines are applied to each column of text, thus duplicate parameter names can exist.  `Keep duplicates=False` hides these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d84f33a9-7da8-43cf-a66f-0f4339785199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>parameter_name</th>\n",
       "      <th>parameter_name_type</th>\n",
       "      <th>current_value</th>\n",
       "      <th>default_value</th>\n",
       "      <th>param_type</th>\n",
       "      <th>supports_grid_search</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>values</th>\n",
       "      <th>parameter_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>analyzer</td>\n",
       "      <td>analyzer_select</td>\n",
       "      <td>word</td>\n",
       "      <td>word</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[word, char]</td>\n",
       "      <td>eyJhcmciOiJhbmFseXplciIsInZpZCI6IjEwIn0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>min_df</td>\n",
       "      <td>min_df_int</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1000000000000000019884624838656</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtaW5fZGYiLCJ2aWQiOiI2In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>use_marisa</td>\n",
       "      <td>use_marisa_select</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJ1c2VfbWFyaXNhIiwidmlkIjoiOCJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>min_tc</td>\n",
       "      <td>min_tc_int</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtaW5fdGMiLCJ2aWQiOiI2In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>n_features</td>\n",
       "      <td>n_features_int</td>\n",
       "      <td>262144</td>\n",
       "      <td>262144</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1048576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJuX2ZlYXR1cmVzIiwidmlkIjoiMTIifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>norm</td>\n",
       "      <td>norm_select</td>\n",
       "      <td>l2</td>\n",
       "      <td>l2</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[None, l1, l2]</td>\n",
       "      <td>eyJhcmciOiJub3JtIiwidmlkIjoiMTAifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>segmenter</td>\n",
       "      <td>segmenter_select</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[None, japanese]</td>\n",
       "      <td>eyJhcmciOiJzZWdtZW50ZXIiLCJ2aWQiOiI4In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>smooth_idf</td>\n",
       "      <td>smooth_idf_select</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJzbW9vdGhfaWRmIiwidmlkIjoiMTAifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>stop_words</td>\n",
       "      <td>stop_words_select</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True, english]</td>\n",
       "      <td>eyJhcmciOiJzdG9wX3dvcmRzIiwidmlkIjoiMTAifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>sublinear_tf</td>\n",
       "      <td>sublinear_tf_select</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJzdWJsaW5lYXJfdGYiLCJ2aWQiOiI2In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>tokenizer</td>\n",
       "      <td>tokenizer_select</td>\n",
       "      <td>sklearn_tokenizer</td>\n",
       "      <td>sklearn_tokenizer</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[sklearn_tokenizer, space, wordpunct, tweet, t...</td>\n",
       "      <td>eyJhcmciOiJ0b2tlbml6ZXIiLCJ2aWQiOiIxMiJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>use_bns</td>\n",
       "      <td>use_bns_select</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJ1c2VfYm5zIiwidmlkIjoiMTAifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>use_char_preprocessing</td>\n",
       "      <td>use_char_preprocessing_select</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJ1c2VfY2hhcl9wcmVwcm9jZXNzaW5nIiwidm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>use_delta</td>\n",
       "      <td>use_delta_select</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJ1c2VfZGVsdGEiLCJ2aWQiOiI2In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>use_hashing</td>\n",
       "      <td>use_hashing_select</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJ1c2VfaGFzaGluZyIsInZpZCI6IjEwIn0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>use_idf</td>\n",
       "      <td>use_idf_select</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJ1c2VfaWRmIiwidmlkIjoiOCJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>max_samples</td>\n",
       "      <td>max_samples_select</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[None]</td>\n",
       "      <td>eyJhcmciOiJtYXhfc2FtcGxlcyIsInZpZCI6IjEwIn0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>max_df</td>\n",
       "      <td>max_df_int</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1000000000000000019884624838656</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtYXhfZGYiLCJ2aWQiOiIxMiJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>use_term_frequency</td>\n",
       "      <td>use_term_frequency_select</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJ1c2VfdGVybV9mcmVxdWVuY3kiLCJ2aWQiOi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>binary</td>\n",
       "      <td>binary_select</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJiaW5hcnkiLCJ2aWQiOiI4In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>decode_error</td>\n",
       "      <td>decode_error_select</td>\n",
       "      <td>replace</td>\n",
       "      <td>replace</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[strict, ignore, replace]</td>\n",
       "      <td>eyJhcmciOiJkZWNvZGVfZXJyb3IiLCJ2aWQiOiI2In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>encoding</td>\n",
       "      <td>encoding_select</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[utf-8, latin-1]</td>\n",
       "      <td>eyJhcmciOiJlbmNvZGluZyIsInZpZCI6IjEwIn0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>lowercase_select</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJsb3dlcmNhc2UiLCJ2aWQiOiIxMiJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>include_lower</td>\n",
       "      <td>include_lower_select</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJpbmNsdWRlX2xvd2VyIiwidmlkIjoiOCJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Auto-Tuned Word N-Gram Text Modeler using toke...</td>\n",
       "      <td>language</td>\n",
       "      <td>language_select</td>\n",
       "      <td>english</td>\n",
       "      <td>english</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[arabic, chinese, danish, dutch, english, finn...</td>\n",
       "      <td>eyJhcmciOiJsYW5ndWFnZSIsInZpZCI6IjYifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>K-Means Clustering</td>\n",
       "      <td>n_clusters</td>\n",
       "      <td>n_clusters_select</td>\n",
       "      <td>auto</td>\n",
       "      <td>auto</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[auto, pham]</td>\n",
       "      <td>eyJhcmciOiJuX2NsdXN0ZXJzIiwidmlkIjoiMTgifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Missing Values Imputed</td>\n",
       "      <td>arbimp</td>\n",
       "      <td>arbimp_int</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>-99999.0000</td>\n",
       "      <td>99999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJhcmJpbXAiLCJ2aWQiOiIyIn0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Missing Values Imputed</td>\n",
       "      <td>min_count_na</td>\n",
       "      <td>min_count_na_int</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>99999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtaW5fY291bnRfbmEiLCJ2aWQiOiIyIn0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>One-Hot Encoding</td>\n",
       "      <td>max_features</td>\n",
       "      <td>max_features_select</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[None]</td>\n",
       "      <td>eyJhcmciOiJtYXhfZmVhdHVyZXMiLCJ2aWQiOiIxNiJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ordinal encoding of categorical variables</td>\n",
       "      <td>card_max</td>\n",
       "      <td>card_max_select</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[None]</td>\n",
       "      <td>eyJhcmciOiJjYXJkX21heCIsInZpZCI6IjEifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Ordinal encoding of categorical variables</td>\n",
       "      <td>min_support</td>\n",
       "      <td>min_support_int</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>99999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtaW5fc3VwcG9ydCIsInZpZCI6IjEifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Ordinal encoding of categorical variables</td>\n",
       "      <td>method</td>\n",
       "      <td>method_select</td>\n",
       "      <td>freq</td>\n",
       "      <td>freq</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[None, random, lex, freq, resp]</td>\n",
       "      <td>eyJhcmciOiJtZXRob2QiLCJ2aWQiOiIxIn0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Partial Principal Components Analysis</td>\n",
       "      <td>k</td>\n",
       "      <td>k_select</td>\n",
       "      <td>auto</td>\n",
       "      <td>auto</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[auto]</td>\n",
       "      <td>eyJhcmciOiJrIiwidmlkIjoiMTcifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>missing_value</td>\n",
       "      <td>missing_value_float</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>float</td>\n",
       "      <td>False</td>\n",
       "      <td>-100000.0000</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtaXNzaW5nX3ZhbHVlIiwidmlkIjoiMTkifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>max_delta_step</td>\n",
       "      <td>max_delta_step_float</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtYXhfZGVsdGFfc3RlcCIsInZpZCI6IjE5In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>base_margin_initialize</td>\n",
       "      <td>base_margin_initialize_select</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>eyJhcmciOiJiYXNlX21hcmdpbl9pbml0aWFsaXplIiwidm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>min_child_weight</td>\n",
       "      <td>min_child_weight_float</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtaW5fY2hpbGRfd2VpZ2h0IiwidmlkIjoiMT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>colsample_bylevel</td>\n",
       "      <td>colsample_bylevel_float</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJjb2xzYW1wbGVfYnlsZXZlbCIsInZpZCI6Ij...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>tree_method</td>\n",
       "      <td>tree_method_select</td>\n",
       "      <td>auto</td>\n",
       "      <td>auto</td>\n",
       "      <td>select</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[auto, exact, approx, hist]</td>\n",
       "      <td>eyJhcmciOiJ0cmVlX21ldGhvZCIsInZpZCI6IjE5In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>colsample_bytree</td>\n",
       "      <td>colsample_bytree_float</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJjb2xzYW1wbGVfYnl0cmVlIiwidmlkIjoiMT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>subsample</td>\n",
       "      <td>subsample_float</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJzdWJzYW1wbGUiLCJ2aWQiOiIxOSJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>max_bin</td>\n",
       "      <td>max_bin_int</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>int</td>\n",
       "      <td>True</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtYXhfYmluIiwidmlkIjoiMTkifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>min_split_loss</td>\n",
       "      <td>min_split_loss_float</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtaW5fc3BsaXRfbG9zcyIsInZpZCI6IjE5In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>smooth_interval</td>\n",
       "      <td>smooth_interval_int</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJzbW9vdGhfaW50ZXJ2YWwiLCJ2aWQiOiIxOSJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>learning_rate</td>\n",
       "      <td>learning_rate_float</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJsZWFybmluZ19yYXRlIiwidmlkIjoiMTkifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>interval</td>\n",
       "      <td>interval_int</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJpbnRlcnZhbCIsInZpZCI6IjE5In0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>scale_pos_weight</td>\n",
       "      <td>scale_pos_weight_float</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1000000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJzY2FsZV9wb3Nfd2VpZ2h0IiwidmlkIjoiMT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>reg_lambda</td>\n",
       "      <td>reg_lambda_float</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJyZWdfbGFtYmRhIiwidmlkIjoiMTkifQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>max_depth</td>\n",
       "      <td>max_depth_int</td>\n",
       "      <td>[3, 5, 7]</td>\n",
       "      <td>3</td>\n",
       "      <td>int</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJtYXhfZGVwdGgiLCJ2aWQiOiIxOSJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>random_state</td>\n",
       "      <td>random_state_int</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>int</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJyYW5kb21fc3RhdGUiLCJ2aWQiOiIxOSJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>num_parallel_tree</td>\n",
       "      <td>num_parallel_tree_int</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>int</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJudW1fcGFyYWxsZWxfdHJlZSIsInZpZCI6Ij...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>n_estimators</td>\n",
       "      <td>n_estimators_int</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>int</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>20000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJuX2VzdGltYXRvcnMiLCJ2aWQiOiIxOSJ9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\n",
       "      <td>reg_alpha</td>\n",
       "      <td>reg_alpha_float</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>float</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eyJhcmciOiJyZWdfYWxwaGEiLCJ2aWQiOiIxOSJ9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            task_name          parameter_name  \\\n",
       "0   Auto-Tuned Word N-Gram Text Modeler using toke...                analyzer   \n",
       "24  Auto-Tuned Word N-Gram Text Modeler using toke...                  min_df   \n",
       "51  Auto-Tuned Word N-Gram Text Modeler using toke...              use_marisa   \n",
       "27  Auto-Tuned Word N-Gram Text Modeler using toke...                  min_tc   \n",
       "31  Auto-Tuned Word N-Gram Text Modeler using toke...              n_features   \n",
       "32  Auto-Tuned Word N-Gram Text Modeler using toke...                    norm   \n",
       "38  Auto-Tuned Word N-Gram Text Modeler using toke...               segmenter   \n",
       "39  Auto-Tuned Word N-Gram Text Modeler using toke...              smooth_idf   \n",
       "41  Auto-Tuned Word N-Gram Text Modeler using toke...              stop_words   \n",
       "42  Auto-Tuned Word N-Gram Text Modeler using toke...            sublinear_tf   \n",
       "44  Auto-Tuned Word N-Gram Text Modeler using toke...               tokenizer   \n",
       "46  Auto-Tuned Word N-Gram Text Modeler using toke...                 use_bns   \n",
       "47  Auto-Tuned Word N-Gram Text Modeler using toke...  use_char_preprocessing   \n",
       "48  Auto-Tuned Word N-Gram Text Modeler using toke...               use_delta   \n",
       "49  Auto-Tuned Word N-Gram Text Modeler using toke...             use_hashing   \n",
       "50  Auto-Tuned Word N-Gram Text Modeler using toke...                 use_idf   \n",
       "20  Auto-Tuned Word N-Gram Text Modeler using toke...             max_samples   \n",
       "18  Auto-Tuned Word N-Gram Text Modeler using toke...                  max_df   \n",
       "52  Auto-Tuned Word N-Gram Text Modeler using toke...      use_term_frequency   \n",
       "3   Auto-Tuned Word N-Gram Text Modeler using toke...                  binary   \n",
       "7   Auto-Tuned Word N-Gram Text Modeler using toke...            decode_error   \n",
       "8   Auto-Tuned Word N-Gram Text Modeler using toke...                encoding   \n",
       "14  Auto-Tuned Word N-Gram Text Modeler using toke...               lowercase   \n",
       "9   Auto-Tuned Word N-Gram Text Modeler using toke...           include_lower   \n",
       "12  Auto-Tuned Word N-Gram Text Modeler using toke...                language   \n",
       "29                                 K-Means Clustering              n_clusters   \n",
       "1                              Missing Values Imputed                  arbimp   \n",
       "23                             Missing Values Imputed            min_count_na   \n",
       "19                                   One-Hot Encoding            max_features   \n",
       "4           Ordinal encoding of categorical variables                card_max   \n",
       "26          Ordinal encoding of categorical variables             min_support   \n",
       "21          Ordinal encoding of categorical variables                  method   \n",
       "11              Partial Principal Components Analysis                       k   \n",
       "28  eXtreme Gradient Boosted Trees Classifier with...           missing_value   \n",
       "16  eXtreme Gradient Boosted Trees Classifier with...          max_delta_step   \n",
       "2   eXtreme Gradient Boosted Trees Classifier with...  base_margin_initialize   \n",
       "22  eXtreme Gradient Boosted Trees Classifier with...        min_child_weight   \n",
       "5   eXtreme Gradient Boosted Trees Classifier with...       colsample_bylevel   \n",
       "45  eXtreme Gradient Boosted Trees Classifier with...             tree_method   \n",
       "6   eXtreme Gradient Boosted Trees Classifier with...        colsample_bytree   \n",
       "43  eXtreme Gradient Boosted Trees Classifier with...               subsample   \n",
       "15  eXtreme Gradient Boosted Trees Classifier with...                 max_bin   \n",
       "25  eXtreme Gradient Boosted Trees Classifier with...          min_split_loss   \n",
       "40  eXtreme Gradient Boosted Trees Classifier with...         smooth_interval   \n",
       "13  eXtreme Gradient Boosted Trees Classifier with...           learning_rate   \n",
       "10  eXtreme Gradient Boosted Trees Classifier with...                interval   \n",
       "37  eXtreme Gradient Boosted Trees Classifier with...        scale_pos_weight   \n",
       "36  eXtreme Gradient Boosted Trees Classifier with...              reg_lambda   \n",
       "17  eXtreme Gradient Boosted Trees Classifier with...               max_depth   \n",
       "34  eXtreme Gradient Boosted Trees Classifier with...            random_state   \n",
       "33  eXtreme Gradient Boosted Trees Classifier with...       num_parallel_tree   \n",
       "30  eXtreme Gradient Boosted Trees Classifier with...            n_estimators   \n",
       "35  eXtreme Gradient Boosted Trees Classifier with...               reg_alpha   \n",
       "\n",
       "              parameter_name_type      current_value      default_value  \\\n",
       "0                 analyzer_select               word               word   \n",
       "24                     min_df_int                  2                  2   \n",
       "51              use_marisa_select               True               True   \n",
       "27                     min_tc_int                  0                  0   \n",
       "31                 n_features_int             262144             262144   \n",
       "32                    norm_select                 l2                 l2   \n",
       "38               segmenter_select               None               None   \n",
       "39              smooth_idf_select               True               True   \n",
       "41              stop_words_select                  0                  0   \n",
       "42            sublinear_tf_select              False              False   \n",
       "44               tokenizer_select  sklearn_tokenizer  sklearn_tokenizer   \n",
       "46                 use_bns_select              False              False   \n",
       "47  use_char_preprocessing_select               True               True   \n",
       "48               use_delta_select              False              False   \n",
       "49             use_hashing_select              False              False   \n",
       "50                 use_idf_select              False              False   \n",
       "20             max_samples_select                500                500   \n",
       "18                     max_df_int                0.8                0.8   \n",
       "52      use_term_frequency_select               True               True   \n",
       "3                   binary_select               True               True   \n",
       "7             decode_error_select            replace            replace   \n",
       "8                 encoding_select              utf-8              utf-8   \n",
       "14               lowercase_select               True               True   \n",
       "9            include_lower_select               True               True   \n",
       "12                language_select            english            english   \n",
       "29              n_clusters_select               auto               auto   \n",
       "1                      arbimp_int              -9999              -9999   \n",
       "23               min_count_na_int                  5                  5   \n",
       "19            max_features_select               None               None   \n",
       "4                 card_max_select               None               None   \n",
       "26                min_support_int                  5                  5   \n",
       "21                  method_select               freq               freq   \n",
       "11                       k_select               auto               auto   \n",
       "28            missing_value_float              -9999              -9999   \n",
       "16           max_delta_step_float                  0                  0   \n",
       "2   base_margin_initialize_select              False              False   \n",
       "22         min_child_weight_float                  1                  1   \n",
       "5         colsample_bylevel_float                  1                  1   \n",
       "45             tree_method_select               auto               auto   \n",
       "6          colsample_bytree_float                0.3                0.3   \n",
       "43                subsample_float                  1                  1   \n",
       "15                    max_bin_int                256                256   \n",
       "25           min_split_loss_float               0.01               0.01   \n",
       "40            smooth_interval_int                200                200   \n",
       "13            learning_rate_float               0.05               0.05   \n",
       "10                   interval_int                 10                 10   \n",
       "37         scale_pos_weight_float                  1                  1   \n",
       "36               reg_lambda_float                  1                  1   \n",
       "17                  max_depth_int          [3, 5, 7]                  3   \n",
       "34               random_state_int                153                153   \n",
       "33          num_parallel_tree_int                  1                  1   \n",
       "30               n_estimators_int               2500               2500   \n",
       "35                reg_alpha_float                  0                  0   \n",
       "\n",
       "   param_type supports_grid_search          min  \\\n",
       "0      select                  NaN          NaN   \n",
       "24        int                False       0.0000   \n",
       "51     select                  NaN          NaN   \n",
       "27        int                False       0.0000   \n",
       "31        int                False       1.0000   \n",
       "32     select                  NaN          NaN   \n",
       "38     select                  NaN          NaN   \n",
       "39     select                  NaN          NaN   \n",
       "41     select                  NaN          NaN   \n",
       "42     select                  NaN          NaN   \n",
       "44     select                  NaN          NaN   \n",
       "46     select                  NaN          NaN   \n",
       "47     select                  NaN          NaN   \n",
       "48     select                  NaN          NaN   \n",
       "49     select                  NaN          NaN   \n",
       "50     select                  NaN          NaN   \n",
       "20     select                  NaN          NaN   \n",
       "18        int                False       0.0000   \n",
       "52     select                  NaN          NaN   \n",
       "3      select                  NaN          NaN   \n",
       "7      select                  NaN          NaN   \n",
       "8      select                  NaN          NaN   \n",
       "14     select                  NaN          NaN   \n",
       "9      select                  NaN          NaN   \n",
       "12     select                  NaN          NaN   \n",
       "29     select                  NaN          NaN   \n",
       "1         int                False  -99999.0000   \n",
       "23        int                False       0.0000   \n",
       "19     select                  NaN          NaN   \n",
       "4      select                  NaN          NaN   \n",
       "26        int                False       1.0000   \n",
       "21     select                  NaN          NaN   \n",
       "11     select                  NaN          NaN   \n",
       "28      float                False -100000.0000   \n",
       "16      float                 True       0.0000   \n",
       "2      select                  NaN          NaN   \n",
       "22      float                 True       0.0100   \n",
       "5       float                 True       0.1000   \n",
       "45     select                  NaN          NaN   \n",
       "6       float                 True       0.0300   \n",
       "43      float                 True       0.0001   \n",
       "15        int                 True      16.0000   \n",
       "25      float                 True       0.0000   \n",
       "40        int                False       2.0000   \n",
       "13      float                 True       0.0005   \n",
       "10        int                False       2.0000   \n",
       "37      float                 True       0.0000   \n",
       "36      float                 True       0.0000   \n",
       "17        int                 True       1.0000   \n",
       "34        int                 True       0.0000   \n",
       "33        int                 True       1.0000   \n",
       "30        int                False       1.0000   \n",
       "35      float                 True       0.0000   \n",
       "\n",
       "                                max  \\\n",
       "0                               NaN   \n",
       "24  1000000000000000019884624838656   \n",
       "51                              NaN   \n",
       "27                              999   \n",
       "31                          1048576   \n",
       "32                              NaN   \n",
       "38                              NaN   \n",
       "39                              NaN   \n",
       "41                              NaN   \n",
       "42                              NaN   \n",
       "44                              NaN   \n",
       "46                              NaN   \n",
       "47                              NaN   \n",
       "48                              NaN   \n",
       "49                              NaN   \n",
       "50                              NaN   \n",
       "20                              NaN   \n",
       "18  1000000000000000019884624838656   \n",
       "52                              NaN   \n",
       "3                               NaN   \n",
       "7                               NaN   \n",
       "8                               NaN   \n",
       "14                              NaN   \n",
       "9                               NaN   \n",
       "12                              NaN   \n",
       "29                              NaN   \n",
       "1                             99999   \n",
       "23                            99999   \n",
       "19                              NaN   \n",
       "4                               NaN   \n",
       "26                            99999   \n",
       "21                              NaN   \n",
       "11                              NaN   \n",
       "28                         100000.0   \n",
       "16                            100.0   \n",
       "2                               NaN   \n",
       "22                         100000.0   \n",
       "5                               1.0   \n",
       "45                              NaN   \n",
       "6                               1.0   \n",
       "43                              1.0   \n",
       "15                             2048   \n",
       "25                         100000.0   \n",
       "40                             1000   \n",
       "13                              1.0   \n",
       "10                              500   \n",
       "37                     1000000000.0   \n",
       "36                        1000000.0   \n",
       "17                               16   \n",
       "34                       1000000000   \n",
       "33                               16   \n",
       "30                            20000   \n",
       "35                        1000000.0   \n",
       "\n",
       "                                               values  \\\n",
       "0                                        [word, char]   \n",
       "24                                                NaN   \n",
       "51                                      [False, True]   \n",
       "27                                                NaN   \n",
       "31                                                NaN   \n",
       "32                                     [None, l1, l2]   \n",
       "38                                   [None, japanese]   \n",
       "39                                      [False, True]   \n",
       "41                             [False, True, english]   \n",
       "42                                      [False, True]   \n",
       "44  [sklearn_tokenizer, space, wordpunct, tweet, t...   \n",
       "46                                      [False, True]   \n",
       "47                                      [False, True]   \n",
       "48                                      [False, True]   \n",
       "49                                      [False, True]   \n",
       "50                                      [False, True]   \n",
       "20                                             [None]   \n",
       "18                                                NaN   \n",
       "52                                      [False, True]   \n",
       "3                                       [False, True]   \n",
       "7                           [strict, ignore, replace]   \n",
       "8                                    [utf-8, latin-1]   \n",
       "14                                      [False, True]   \n",
       "9                                       [False, True]   \n",
       "12  [arabic, chinese, danish, dutch, english, finn...   \n",
       "29                                       [auto, pham]   \n",
       "1                                                 NaN   \n",
       "23                                                NaN   \n",
       "19                                             [None]   \n",
       "4                                              [None]   \n",
       "26                                                NaN   \n",
       "21                    [None, random, lex, freq, resp]   \n",
       "11                                             [auto]   \n",
       "28                                                NaN   \n",
       "16                                                NaN   \n",
       "2                                       [False, True]   \n",
       "22                                                NaN   \n",
       "5                                                 NaN   \n",
       "45                        [auto, exact, approx, hist]   \n",
       "6                                                 NaN   \n",
       "43                                                NaN   \n",
       "15                                                NaN   \n",
       "25                                                NaN   \n",
       "40                                                NaN   \n",
       "13                                                NaN   \n",
       "10                                                NaN   \n",
       "37                                                NaN   \n",
       "36                                                NaN   \n",
       "17                                                NaN   \n",
       "34                                                NaN   \n",
       "33                                                NaN   \n",
       "30                                                NaN   \n",
       "35                                                NaN   \n",
       "\n",
       "                                         parameter_id  \n",
       "0             eyJhcmciOiJhbmFseXplciIsInZpZCI6IjEwIn0  \n",
       "24                eyJhcmciOiJtaW5fZGYiLCJ2aWQiOiI2In0  \n",
       "51           eyJhcmciOiJ1c2VfbWFyaXNhIiwidmlkIjoiOCJ9  \n",
       "27                eyJhcmciOiJtaW5fdGMiLCJ2aWQiOiI2In0  \n",
       "31         eyJhcmciOiJuX2ZlYXR1cmVzIiwidmlkIjoiMTIifQ  \n",
       "32                 eyJhcmciOiJub3JtIiwidmlkIjoiMTAifQ  \n",
       "38            eyJhcmciOiJzZWdtZW50ZXIiLCJ2aWQiOiI4In0  \n",
       "39         eyJhcmciOiJzbW9vdGhfaWRmIiwidmlkIjoiMTAifQ  \n",
       "41         eyJhcmciOiJzdG9wX3dvcmRzIiwidmlkIjoiMTAifQ  \n",
       "42        eyJhcmciOiJzdWJsaW5lYXJfdGYiLCJ2aWQiOiI2In0  \n",
       "44           eyJhcmciOiJ0b2tlbml6ZXIiLCJ2aWQiOiIxMiJ9  \n",
       "46             eyJhcmciOiJ1c2VfYm5zIiwidmlkIjoiMTAifQ  \n",
       "47  eyJhcmciOiJ1c2VfY2hhcl9wcmVwcm9jZXNzaW5nIiwidm...  \n",
       "48            eyJhcmciOiJ1c2VfZGVsdGEiLCJ2aWQiOiI2In0  \n",
       "49        eyJhcmciOiJ1c2VfaGFzaGluZyIsInZpZCI6IjEwIn0  \n",
       "50               eyJhcmciOiJ1c2VfaWRmIiwidmlkIjoiOCJ9  \n",
       "20        eyJhcmciOiJtYXhfc2FtcGxlcyIsInZpZCI6IjEwIn0  \n",
       "18               eyJhcmciOiJtYXhfZGYiLCJ2aWQiOiIxMiJ9  \n",
       "52  eyJhcmciOiJ1c2VfdGVybV9mcmVxdWVuY3kiLCJ2aWQiOi...  \n",
       "3                 eyJhcmciOiJiaW5hcnkiLCJ2aWQiOiI4In0  \n",
       "7         eyJhcmciOiJkZWNvZGVfZXJyb3IiLCJ2aWQiOiI2In0  \n",
       "8             eyJhcmciOiJlbmNvZGluZyIsInZpZCI6IjEwIn0  \n",
       "14           eyJhcmciOiJsb3dlcmNhc2UiLCJ2aWQiOiIxMiJ9  \n",
       "9        eyJhcmciOiJpbmNsdWRlX2xvd2VyIiwidmlkIjoiOCJ9  \n",
       "12             eyJhcmciOiJsYW5ndWFnZSIsInZpZCI6IjYifQ  \n",
       "29         eyJhcmciOiJuX2NsdXN0ZXJzIiwidmlkIjoiMTgifQ  \n",
       "1                 eyJhcmciOiJhcmJpbXAiLCJ2aWQiOiIyIn0  \n",
       "23        eyJhcmciOiJtaW5fY291bnRfbmEiLCJ2aWQiOiIyIn0  \n",
       "19       eyJhcmciOiJtYXhfZmVhdHVyZXMiLCJ2aWQiOiIxNiJ9  \n",
       "4              eyJhcmciOiJjYXJkX21heCIsInZpZCI6IjEifQ  \n",
       "26         eyJhcmciOiJtaW5fc3VwcG9ydCIsInZpZCI6IjEifQ  \n",
       "21                eyJhcmciOiJtZXRob2QiLCJ2aWQiOiIxIn0  \n",
       "11                     eyJhcmciOiJrIiwidmlkIjoiMTcifQ  \n",
       "28     eyJhcmciOiJtaXNzaW5nX3ZhbHVlIiwidmlkIjoiMTkifQ  \n",
       "16    eyJhcmciOiJtYXhfZGVsdGFfc3RlcCIsInZpZCI6IjE5In0  \n",
       "2   eyJhcmciOiJiYXNlX21hcmdpbl9pbml0aWFsaXplIiwidm...  \n",
       "22  eyJhcmciOiJtaW5fY2hpbGRfd2VpZ2h0IiwidmlkIjoiMT...  \n",
       "5   eyJhcmciOiJjb2xzYW1wbGVfYnlsZXZlbCIsInZpZCI6Ij...  \n",
       "45        eyJhcmciOiJ0cmVlX21ldGhvZCIsInZpZCI6IjE5In0  \n",
       "6   eyJhcmciOiJjb2xzYW1wbGVfYnl0cmVlIiwidmlkIjoiMT...  \n",
       "43           eyJhcmciOiJzdWJzYW1wbGUiLCJ2aWQiOiIxOSJ9  \n",
       "15             eyJhcmciOiJtYXhfYmluIiwidmlkIjoiMTkifQ  \n",
       "25    eyJhcmciOiJtaW5fc3BsaXRfbG9zcyIsInZpZCI6IjE5In0  \n",
       "40   eyJhcmciOiJzbW9vdGhfaW50ZXJ2YWwiLCJ2aWQiOiIxOSJ9  \n",
       "13     eyJhcmciOiJsZWFybmluZ19yYXRlIiwidmlkIjoiMTkifQ  \n",
       "10            eyJhcmciOiJpbnRlcnZhbCIsInZpZCI6IjE5In0  \n",
       "37  eyJhcmciOiJzY2FsZV9wb3Nfd2VpZ2h0IiwidmlkIjoiMT...  \n",
       "36         eyJhcmciOiJyZWdfbGFtYmRhIiwidmlkIjoiMTkifQ  \n",
       "17           eyJhcmciOiJtYXhfZGVwdGgiLCJ2aWQiOiIxOSJ9  \n",
       "34       eyJhcmciOiJyYW5kb21fc3RhdGUiLCJ2aWQiOiIxOSJ9  \n",
       "33  eyJhcmciOiJudW1fcGFyYWxsZWxfdHJlZSIsInZpZCI6Ij...  \n",
       "30       eyJhcmciOiJuX2VzdGltYXRvcnMiLCJ2aWQiOiIxOSJ9  \n",
       "35           eyJhcmciOiJyZWdfYWxwaGEiLCJ2aWQiOiIxOSJ9  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_to_df(model.get_advanced_tuning_parameters(), keep_duplicates='first') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5385e024-cf27-4be9-aa00-e9cd354c7ac5",
   "metadata": {},
   "source": [
    "## Run an advanced tuning session\n",
    "\n",
    "The blueprint processes shown above are available as tunable tasks. To perform a hyperparameter search, you must first instantiate an advanced tuning session on your selected model. You will run a single tuning session below.\n",
    "\n",
    "The example below: \n",
    "- Starts and advanced Tuning Session\n",
    "- Lists the tunable tasks (the first column in the dataframe above)\n",
    "- Executes a tuning job on the `colsample_bytree` and `learning_rate_float` hyperparameters on the learning algorithm\n",
    "\n",
    "### Create an advanced tuning session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a5aeb1a-1f5d-4f15-b38e-499dee882759",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATS = dr.models.advanced_tuning.AdvancedTuningSession(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea482575-a4dd-48c7-b0c0-09eae6f3a0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Auto-Tuned Word N-Gram Text Modeler using token occurrences',\n",
       " 'K-Means Clustering',\n",
       " 'Missing Values Imputed',\n",
       " 'One-Hot Encoding',\n",
       " 'Ordinal encoding of categorical variables',\n",
       " 'Partial Principal Components Analysis',\n",
       " 'eXtreme Gradient Boosted Trees Classifier with Early Stopping and Unsupervised Learning Features']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks = ATS.get_task_names()\n",
    "tasks\n",
    "# ATS.get_parameter_names(tasks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94cf8aab-7aa1-40eb-9651-81cf718baded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LogLoss': {'validation': 0.36724,\n",
       "  'crossValidation': 0.357612,\n",
       "  'holdout': None,\n",
       "  'training': None,\n",
       "  'backtestingScores': None,\n",
       "  'backtesting': None}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATS.set_parameter(parameter_name=\"colsample_bytree\", value=0.4)\n",
    "ATS.set_parameter(parameter_name=\"learning_rate\", value=0.045)\n",
    "job = ATS.run()\n",
    "tuned_model = job.get_result_when_complete()\n",
    "{model.project.metric: tuned_model.metrics[model.project.metric]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f18601-f6c1-48bb-9cc5-7f19c4c3ee9c",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with grid search\n",
    "\n",
    "The approach above is sufficient for modifying exact values, and now you'll expand to a custom grid search. In DataRobot, grid searching hyperparameters can be done in two ways:\n",
    "\n",
    "1. Testing multiple hyperparameter combinations within a single model\n",
    "\n",
    "   - Uses internal partition for hyperparameter selection (note: these are not the Leaderboard partitions)\n",
    "   - Uses DataRobot's proprietary version of [pattern search](https://app.datarobot.com/docs/modeling/analyze-models/evaluate/adv-tuning.html#set-the-search-type) for optimization\n",
    "   - Useful if:\n",
    "       * You want hyperparameters to be chosen using the project metric\n",
    "       * You don't want the Leaderboard partitions to be used for hyperparameter selection (best practice and default behavior for DataRobot)\n",
    "       \n",
    "   \n",
    "2. Testing multiple hyperparameter combinations with multiple models\n",
    "\n",
    "   - Uses a leaderboard partition for hyperparameter selection\n",
    "   - Can use any optimization strategy of interest (e.g., brute force, Bayesian Optimization, etc.)\n",
    "   - Useful if:\n",
    "       1. You want hyperparameters to be chosen based on a Leaderboard (or custom) metric\n",
    "       2. You want to use a Leaderboard partition for hyperparameter selection (be sure to use a different partition for model evaluation)\n",
    "       3. You want to search over hyperparameters that _do not_ accept one or more values (see [docs](https://app.datarobot.com/docs/modeling/analyze-models/evaluate/adv-tuning.html#set-a-parameter)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548478f5-8c35-44ca-99a8-8028dc9ed621",
   "metadata": {},
   "source": [
    "### Option 1: Grid search internally with pattern search model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f773abe9-a24d-4c30-b20d-89816bd4d89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample: 0.5\n",
      "colsample_bytree: 0.4\n",
      "max_depth: 4\n",
      "CPU times: user 187 ms, sys: 21.7 ms, total: 209 ms\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Option (1)\n",
    "ATS = dr.models.advanced_tuning.AdvancedTuningSession(model)\n",
    "ATS.set_parameter(parameter_name=\"subsample\", value=[0.5, 0.7, 1])\n",
    "ATS.set_parameter(parameter_name=\"colsample_bytree\", value=[0.4, 0.6, 0.8, 1])\n",
    "ATS.set_parameter(parameter_name=\"max_depth\", value=[4, 6, 8, 10])\n",
    "job = ATS.run()\n",
    "tuned_model = job.get_result_when_complete()\n",
    "{model.project.metric: tuned_model.metrics[model.project.metric]}\n",
    "\n",
    "# See the best values\n",
    "tuned_hyperparameters = tuned_model.get_advanced_tuning_parameters()[\"tuning_parameters\"]\n",
    "for hp in [\"subsample\", \"colsample_bytree\", \"max_depth\"]:\n",
    "    \n",
    "    hp_values = [x for x in tuned_hyperparameters if x[\"parameter_name\"] == hp][0]\n",
    "    print(f\"{hp}: {hp_values['default_value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856d390a-1b06-4993-acf7-7fcae977fa93",
   "metadata": {},
   "source": [
    "### Option 2: Brute force grid search with one model per combination on project validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63a21f2f-ca55-4b53-8288-66d7fe36182b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations to evaluate: 48\n",
      "Waiting for 48 models...\n",
      "48 completed successfully!\n",
      "Deleting 43 of 48 models...\n",
      "Model deletion finished!\n",
      "CPU times: user 4.04 s, sys: 510 ms, total: 4.55 s\n",
      "Wall time: 8min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Option (2)\n",
    "# Tuning over same grid\n",
    "validation_type = 'validation'\n",
    "\n",
    "tuning_hyperparameters(model=model, \n",
    "                       advanced_tuning_grid={\"subsample\": [0.5, 0.7, 1],\n",
    "                                             \"colsample_bytree\": [0.4, 0.6, 0.8, 1],\n",
    "                                             \"max_depth\": [4, 6, 8, 10]},\n",
    "                       partition= validation_type,\n",
    "                       metric=None, # Will default to project metric\n",
    "                       max_n_models_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a66965b-5ac4-4778-a11c-cfc848d53ebf",
   "metadata": {},
   "source": [
    "### Extra credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10f8c73c-ede3-4ab9-9c08-8c92450d256d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422 client error: {'message': 'Only one value allowed for this parameter'}\n",
      "\n",
      "\n",
      "Number of hyperparameter combinations to evaluate: 3\n",
      "Waiting for 3 models...\n",
      "3 completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Tuning over a hyperparameter that cannot accept more than 1 value\n",
    "try:\n",
    "\n",
    "    ATS = dr.models.advanced_tuning.AdvancedTuningSession(model)\n",
    "    ATS.set_parameter(parameter_name=\"smooth_interval\", value=[50, 100, 200])\n",
    "    job = ATS.run()\n",
    "\n",
    "except Exception as error:\n",
    "\n",
    "    print(error)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Can tune over this hyperparameter here because 3 different models will be built\n",
    "tuning_hyperparameters(\n",
    "    model=model,\n",
    "    advanced_tuning_grid={\"smooth_interval\": [50, 100, 200]},\n",
    "    partition=validation_type,\n",
    "    metric=None,\n",
    "    max_n_models_to_keep=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cda57f-efcb-42c4-8d88-6b3af71bfe71",
   "metadata": {},
   "source": [
    "## Tuning over preprocessing steps and model hyperparameters\n",
    "\n",
    "The cell below applies the brute force approach to multiple steps in the modeling pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c071c0b4-192d-4442-810a-e579f9db9e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations to evaluate: 144\n",
      "Waiting for 144 models...\n",
      "144 completed successfully!\n",
      "Deleting 139 of 144 models...\n",
      "Model deletion finished!\n",
      "CPU times: user 10.4 s, sys: 1.24 s, total: 11.6 s\n",
      "Wall time: 20min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tuning over a mix of preprocessing and model hyperparameters\n",
    "tuning_hyperparameters(model=model, \n",
    "                       advanced_tuning_grid={\"method\": [\"resp\", \"freq\", \"lex\"], #ordinal encoding\n",
    "                                             \"binary\": [True, False], #text pipeline\n",
    "                                             \"analyzer\": ['word', 'char'],#text pipeline\n",
    "                                             \"tree_method\":[\"exact\", \"approx\", \"hist\"],\n",
    "                                             \"max_depth\": [4, 6, 8, 10]},\n",
    "                       partition= 'validation',\n",
    "                       metric=None,\n",
    "                       max_n_models_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c0e76-28fb-400d-ab3f-087e976491df",
   "metadata": {},
   "source": [
    "`get_tuned_parameters` returns the original model from the beginning of the notebook at index 0, and then the other tuned runs for comparison, along with the parameters that differed from the original model in each subsequent run. Below we sort by loss, so we can see that one run showed improvement over our initial model (index 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49f52fb3-ea8c-4c6e-b418-f037b1f9b37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar models found:  14\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>colsample_bytree_float</th>\n",
       "      <th>learning_rate_float</th>\n",
       "      <th>max_depth_int</th>\n",
       "      <th>method_select</th>\n",
       "      <th>smooth_interval_int</th>\n",
       "      <th>subsample_float</th>\n",
       "      <th>tree_method_select</th>\n",
       "      <th>validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63ddc6d4c95ec74fc19be627</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>freq</td>\n",
       "      <td>200</td>\n",
       "      <td>0.7</td>\n",
       "      <td>auto</td>\n",
       "      <td>0.3654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63dd8e84f8dd7f9f66de9c1b</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3</td>\n",
       "      <td>freq</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>0.36571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>63ddc8be043a16aed62addb0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3</td>\n",
       "      <td>freq</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>0.36571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>63ddc8c136df624f4811143d</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3</td>\n",
       "      <td>freq</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>0.36571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>63ddc8c42e5a3f329211152d</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3</td>\n",
       "      <td>freq</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>0.36571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model_id colsample_bytree_float learning_rate_float  \\\n",
       "3   63ddc6d4c95ec74fc19be627                    0.4                0.05   \n",
       "0   63dd8e84f8dd7f9f66de9c1b                    0.3                0.05   \n",
       "8   63ddc8be043a16aed62addb0                    0.3                0.05   \n",
       "9   63ddc8c136df624f4811143d                    0.3                0.05   \n",
       "10  63ddc8c42e5a3f329211152d                    0.3                0.05   \n",
       "\n",
       "   max_depth_int method_select smooth_interval_int subsample_float  \\\n",
       "3              4          freq                 200             0.7   \n",
       "0              3          freq                 200               1   \n",
       "8              3          freq                  50               1   \n",
       "9              3          freq                 100               1   \n",
       "10             3          freq                 200               1   \n",
       "\n",
       "   tree_method_select validation  \n",
       "3                auto     0.3654  \n",
       "0                auto    0.36571  \n",
       "8                auto    0.36571  \n",
       "9                auto    0.36571  \n",
       "10               auto    0.36571  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_param = get_tuned_parameters(project, model, validation_type)\n",
    "tuned_param.sort_values(by=\"validation\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c5d12e37-408b-4010-88e5-f00cd924ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the max improvement by partition from all tunes\n",
    "def perc_imp(df,model, val=\"validation\"):\n",
    "    out = ((df.loc[df[\"model_id\"] == model.id][\n",
    "                                             val\n",
    "                                             ] / df[val].min())-1)*100\n",
    "    return round(out[0],4)\n",
    "# Return the total number of tunes that performed better than the base model by partition\n",
    "def num_improved(df, val=\"validation\"):\n",
    "    out =df[df[val]<df.loc[0][val]].shape[0]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ef52bbc3-2631-4fb1-b94b-ed66e9aab6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  of the tuning runs improved over the baseline run from the project, with the best tune yielding 0.0848 % improvement on validation, which is in line with the gains we discussed at the beginning\n"
     ]
    }
   ],
   "source": [
    "print(num_improved(tuned_param),\" of the tuning runs improved over the baseline run from the project, with the best tune yielding\", perc_imp(tuned_param,model),\"% improvement on validation, which is in line with the gains we discussed at the beginning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f2b31b-a39c-489b-b455-c22e083a3187",
   "metadata": {},
   "source": [
    "Note the neglible improvement from the search space provided. Learning rate was left out previously--explore that below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ce7f4d77-39b8-4d52-87fd-15537a410c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations to evaluate: 24\n",
      "Waiting for 24 models...\n",
      "24 completed successfully!\n",
      "Deleting 19 of 24 models...\n",
      "Model deletion finished!\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "# Add learning rate to search space\n",
    "validation_type = 'validation'\n",
    "\n",
    "tuning_hyperparameters(model=model, \n",
    "                       advanced_tuning_grid={ \"learning_rate\":[0.02,0.03,0.04],\n",
    "                                              \"subsample\": [0.5,0.7],\n",
    "                                              \"colsample_bytree\": [0.4,0.6],\n",
    "                                              \"max_depth\": [4,6]},\n",
    "                       partition= validation_type,\n",
    "                       metric=None, # Will default to project metric\n",
    "                       max_n_models_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "567df90c-bd53-4ff8-9c34-177ca83f3080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar models found:  19\n",
      "0.0848 % improvement\n",
      "1  model with less error than the base model\n"
     ]
    }
   ],
   "source": [
    "# Check results after last tune\n",
    "tuned_param2 = get_tuned_parameters(project, model, validation_type)\n",
    "print(perc_imp(tuned_param2,model), \"% improvement\")\n",
    "print(num_improved(tuned_param2),\" model with less error than the base model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d45d4223-1dc3-46dd-82af-34f9382b52e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar models found:  19\n",
      "0.291 % improvement\n",
      "11  model with less error than the base model\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance on cross-validation of tuned-models\n",
    "tuned_param3 = get_tuned_parameters(project, model, \"crossValidation\")\n",
    "print(perc_imp(tuned_param3,model,val=\"crossValidation\"), \"% improvement\")\n",
    "print(num_improved(tuned_param3, val=\"crossValidation\"),\" model with less error than the base model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a563427-a888-4e6a-a6d1-3cab20063a4f",
   "metadata": {},
   "source": [
    "After 100+ runs, there was close to neglible improvement over our baseline model.  We can see that the large experiment tuning over the text pipeline with binarization, word/character encoding, and alternative ordinal encodings for the categoricals didn't make a difference, and that can give us confidence in that specific part of the original model pipeline. It is also a nod towards the heuristics and pattern search approach employed out of the box with DataRobot. \n",
    "\n",
    "At the same time, we only evaluated a few settings in this example. Approaches that leverage hyperparameter sampling across a range of continuous features, random search methods, as well as your own preferred tuning approaches are other options you can implement from the code provided here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2298d9-9bda-4d33-8546-1a93832ac83c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "The steps above provide a repeatable process for accessing and tuning DataRobot models, as well as introduce concepts around Blueprint creation and selection. \n",
    "\n",
    "An additional brute force grid search was presented, which enables you to tune based on the project or a custom metric.  A modest lift of <1% on validation and cross-validation was achieved, which is in line with the heuristic presented at the beginning where hyperparameter tuning often yields in the low single digit percent improvements in error. Another takeaway with this approach is that there is no incremental learning occurring between tuning rounds, and performance is limited to user knowledge of hyperparameter values. \n",
    "\n",
    "In practice, if you know the parameters you want to try, a simple brute force approach or pattern search implemented by DataRobot works fine. However, the increased computation and time associated with having one model to evaluate each combination. This obviously explodes across a wide search space, and is not optimized for computation or time. When those two constraints matter, approaches such as Bayesian Optimization can be beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c74a6-e523-47ce-95bc-05136108406a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
