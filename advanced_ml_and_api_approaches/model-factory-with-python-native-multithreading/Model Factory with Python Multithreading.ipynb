{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dbe9c15",
   "metadata": {},
   "source": [
    "# Build a model factory with Python multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b49e7d",
   "metadata": {},
   "source": [
    "Author: Pavel Ustinov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6a8af2",
   "metadata": {},
   "source": [
    "July 5th, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e5ffbf",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9efc14b",
   "metadata": {},
   "source": [
    "It is [well-known](https://docs.python.org/3/library/threading.html) that only one thread can execute Python code at once in CPython (even though certain performance-oriented libraries might overcome this limitation) because of the Global Interpreter Lock ([GIL](https://docs.python.org/3/glossary.html#term-global-interpreter-lock)). Despite this disadvantage, multithreading is still an appropriate approach if you want to run multiple I/O-bound tasks simultaneously.\n",
    "\n",
    "The DataRobot platform makes it possible to create model factories. A model factory is a system or set of procedures that automatically generate predictive models with little to no human intervention. More details can be found [here](https://docs.datarobot.com/en/docs/api/guide/python/Build-a-Model-Factory.html). The third party frameworks from the Python ecosystem can also be used for model factories' building (for example, [Dask](https://docs.dask.org/en/stable/)). One of the [best Dask practices](https://docs.dask.org/en/stable/best-practices.html#stop-using-dask-when-no-longer-needed) is not to overuse Dask when its *distributed parallelism* is not really needed, especially if you don't use large amount of data.\n",
    "\n",
    "The application of model factories improves the throughput of DataRobot cluster over models' training phase leveraging better usage of the DataRobot modeling workers. That allows decreasing the training time of the models increasing the efficiency of data science teams who need to train *tens* and *hundreds* of different models. Usually the performance gain can reach **2-3 times** for the training time in comparison to the sequential project training.\n",
    "\n",
    "This accelerator shows a simple example of how to use the Python *threading* library to build a model factory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f01b6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b16f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.1\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import concurrent.futures as f\n",
    "import datarobot as dr\n",
    "from datarobot import AUTOPILOT_MODE\n",
    "print(dr.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef78acf",
   "metadata": {},
   "source": [
    "Set the number of pool workers and the model target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d1efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "THREAD_POOL_WORKERS = 5\n",
    "TARGET = 'SalePrice'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfbd1a8",
   "metadata": {},
   "source": [
    "### Connect to DataRobot\n",
    "\n",
    "Read more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5890e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datarobot.rest.RESTClientObject at 0x103479c90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr.Client(config_path='drconfig.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c5d18a",
   "metadata": {},
   "source": [
    "### Create a dataset in the AI Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b040a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_file_path = 'https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/house_train_dataset.csv'\n",
    "training_dataset = dr.Dataset.create_from_url(training_dataset_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90b7db8",
   "metadata": {},
   "source": [
    "### Create a DataRobot project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b48f4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = dr.Project.create_from_dataset(training_dataset.id, project_name='Sequential Project')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc7fd6b",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Start Autopilot for one project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c566643e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project(Sequential Project)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.analyze_and_model(target=TARGET, worker_count=-1, mode=AUTOPILOT_MODE.QUICK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a22d613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In progress: 8, queued: 0 (waited: 0s)\n",
      "In progress: 8, queued: 0 (waited: 1s)\n",
      "In progress: 8, queued: 0 (waited: 2s)\n",
      "In progress: 8, queued: 0 (waited: 3s)\n",
      "In progress: 8, queued: 0 (waited: 5s)\n",
      "In progress: 8, queued: 0 (waited: 7s)\n",
      "In progress: 8, queued: 0 (waited: 11s)\n",
      "In progress: 8, queued: 0 (waited: 18s)\n",
      "In progress: 8, queued: 0 (waited: 31s)\n",
      "In progress: 0, queued: 0 (waited: 58s)\n",
      "In progress: 0, queued: 0 (waited: 110s)\n",
      "In progress: 1, queued: 0 (waited: 170s)\n",
      "In progress: 0, queued: 0 (waited: 231s)\n",
      "In progress: 0, queued: 0 (waited: 292s)\n"
     ]
    }
   ],
   "source": [
    "project.wait_for_autopilot(check_interval=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb5ad15",
   "metadata": {},
   "source": [
    "### Start Autopilot for one project with advanced options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d8acb",
   "metadata": {},
   "source": [
    "You can decrease training time if there is no need to prepare model for the deployment and train blenders. It can be useful during the ML experimentation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a4496fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_options = dr.AdvancedOptions(prepare_model_for_deployment=False, blend_best_models=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5357bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = dr.Project.create_from_dataset(training_dataset.id, project_name='Sequential Project (advanced options)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e1d45dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project(Sequential Project (advanced options))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.analyze_and_model(target=TARGET, worker_count=-1, mode=AUTOPILOT_MODE.QUICK, advanced_options=advanced_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c8074ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In progress: 8, queued: 0 (waited: 0s)\n",
      "In progress: 8, queued: 0 (waited: 1s)\n",
      "In progress: 8, queued: 0 (waited: 2s)\n",
      "In progress: 8, queued: 0 (waited: 3s)\n",
      "In progress: 8, queued: 0 (waited: 4s)\n",
      "In progress: 8, queued: 0 (waited: 7s)\n",
      "In progress: 8, queued: 0 (waited: 11s)\n",
      "In progress: 8, queued: 0 (waited: 18s)\n",
      "In progress: 8, queued: 0 (waited: 31s)\n",
      "In progress: 2, queued: 0 (waited: 58s)\n",
      "In progress: 2, queued: 0 (waited: 110s)\n",
      "In progress: 0, queued: 0 (waited: 170s)\n"
     ]
    }
   ],
   "source": [
    "project.wait_for_autopilot(check_interval=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60d7a6",
   "metadata": {},
   "source": [
    "You can see that the training time decreased from **292s** to **170s** (**42%** gain)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7af96",
   "metadata": {},
   "source": [
    "### Modeling five projects in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f0ca5",
   "metadata": {},
   "source": [
    "Create a list with five DataRobot projects that will be trained in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "055e4ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Project(Parallel Project - 1), Project(Parallel Project - 2), Project(Parallel Project - 3), Project(Parallel Project - 4), Project(Parallel Project - 5)]\n"
     ]
    }
   ],
   "source": [
    "project_list = []\n",
    "for n in range(1, 6):\n",
    "    project_name = f'Parallel Project - {n}'\n",
    "    project = dr.Project.create_from_dataset(training_dataset.id, project_name=project_name)\n",
    "    project_list.append(project)\n",
    "print(project_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f447e",
   "metadata": {},
   "source": [
    "This function kicks off an independent training process for every project (5 projects created in this example) in each thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f1551d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_function(project, start_time):\n",
    "    print(f\"Start training of project '{project.project_name}'...\\n\")\n",
    "    project.analyze_and_model(target=TARGET, worker_count=-1, mode=AUTOPILOT_MODE.QUICK, max_wait=14400)\n",
    "    project.wait_for_autopilot(check_interval=60)\n",
    "    \n",
    "    return datetime.datetime.now() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aff80d",
   "metadata": {},
   "source": [
    "### Submit tasks to executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c125b592",
   "metadata": {},
   "source": [
    "The **ThreadPoolExecutor** [subclass](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor) with the predefined number of threads will be used to submit tasks for the asynchronous execution. The context manager should be used for the correct resources' management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0f1e6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training of project 'Parallel Project - 1'...\n",
      "\n",
      "Start training of project 'Parallel Project - 2'...\n",
      "\n",
      "Start training of project 'Parallel Project - 3'...\n",
      "\n",
      "Start training of project 'Parallel Project - 4'...\n",
      "\n",
      "Start training of project 'Parallel Project - 5'...\n",
      "\n",
      "In progress: 8, queued: 0 (waited: 0s)\n",
      "In progress: 8, queued: 0 (waited: 0s)\n",
      "In progress: 4, queued: 4 (waited: 0s)\n",
      "In progress: 8, queued: 0 (waited: 1s)\n",
      "In progress: 8, queued: 0 (waited: 1s)\n",
      "In progress: 4, queued: 4 (waited: 1s)\n",
      "In progress: 8, queued: 0 (waited: 2s)\n",
      "In progress: 8, queued: 0 (waited: 2s)\n",
      "In progress: 4, queued: 4 (waited: 2s)\n",
      "In progress: 8, queued: 0 (waited: 3s)\n",
      "In progress: 8, queued: 0 (waited: 3s)\n",
      "In progress: 4, queued: 4 (waited: 3s)\n",
      "In progress: 8, queued: 0 (waited: 4s)\n",
      "In progress: 8, queued: 0 (waited: 5s)\n",
      "In progress: 4, queued: 4 (waited: 4s)\n",
      "In progress: 0, queued: 8 (waited: 0s)\n",
      "In progress: 0, queued: 8 (waited: 0s)\n",
      "In progress: 0, queued: 8 (waited: 1s)\n",
      "In progress: 0, queued: 8 (waited: 1s)\n",
      "In progress: 8, queued: 0 (waited: 7s)\n",
      "In progress: 8, queued: 0 (waited: 7s)\n",
      "In progress: 4, queued: 4 (waited: 7s)\n",
      "In progress: 0, queued: 8 (waited: 2s)\n",
      "In progress: 0, queued: 8 (waited: 2s)\n",
      "In progress: 0, queued: 8 (waited: 3s)\n",
      "In progress: 0, queued: 8 (waited: 3s)\n",
      "In progress: 0, queued: 8 (waited: 4s)\n",
      "In progress: 0, queued: 8 (waited: 4s)\n",
      "In progress: 8, queued: 0 (waited: 11s)\n",
      "In progress: 8, queued: 0 (waited: 11s)\n",
      "In progress: 4, queued: 4 (waited: 10s)\n",
      "In progress: 0, queued: 8 (waited: 7s)\n",
      "In progress: 0, queued: 8 (waited: 7s)\n",
      "In progress: 0, queued: 8 (waited: 11s)\n",
      "In progress: 0, queued: 8 (waited: 11s)\n",
      "In progress: 8, queued: 0 (waited: 18s)\n",
      "In progress: 8, queued: 0 (waited: 18s)\n",
      "In progress: 4, queued: 4 (waited: 18s)\n",
      "In progress: 0, queued: 8 (waited: 18s)\n",
      "In progress: 0, queued: 8 (waited: 18s)\n",
      "In progress: 8, queued: 0 (waited: 31s)\n",
      "In progress: 8, queued: 0 (waited: 31s)\n",
      "In progress: 4, queued: 4 (waited: 31s)\n",
      "In progress: 0, queued: 8 (waited: 31s)\n",
      "In progress: 0, queued: 8 (waited: 31s)\n",
      "In progress: 1, queued: 0 (waited: 57s)\n",
      "In progress: 1, queued: 0 (waited: 58s)\n",
      "In progress: 5, queued: 0 (waited: 57s)\n",
      "In progress: 8, queued: 0 (waited: 57s)\n",
      "In progress: 5, queued: 3 (waited: 57s)\n",
      "In progress: 6, queued: 10 (waited: 109s)\n",
      "In progress: 0, queued: 16 (waited: 110s)\n",
      "In progress: 0, queued: 0 (waited: 109s)\n",
      "In progress: 3, queued: 0 (waited: 109s)\n",
      "In progress: 1, queued: 0 (waited: 109s)\n",
      "In progress: 6, queued: 0 (waited: 170s)\n",
      "In progress: 0, queued: 0 (waited: 170s)\n",
      "In progress: 10, queued: 6 (waited: 170s)\n",
      "In progress: 0, queued: 16 (waited: 170s)\n",
      "In progress: 0, queued: 16 (waited: 170s)\n",
      "In progress: 0, queued: 0 (waited: 231s)\n",
      "In progress: 0, queued: 0 (waited: 231s)\n",
      "In progress: 0, queued: 0 (waited: 231s)\n",
      "In progress: 2, queued: 0 (waited: 231s)\n",
      "In progress: 14, queued: 0 (waited: 231s)\n",
      "In progress: 0, queued: 0 (waited: 292s)\n",
      "In progress: 0, queued: 0 (waited: 292s)\n",
      "In progress: 0, queued: 0 (waited: 291s)\n",
      "In progress: 0, queued: 0 (waited: 291s)\n",
      "In progress: 0, queued: 0 (waited: 292s)\n",
      "In progress: 1, queued: 0 (waited: 352s)\n",
      "In progress: 1, queued: 0 (waited: 352s)\n",
      "In progress: 1, queued: 0 (waited: 352s)\n",
      "In progress: 1, queued: 0 (waited: 352s)\n",
      "In progress: 1, queued: 0 (waited: 352s)\n",
      "In progress: 0, queued: 0 (waited: 413s)\n",
      "In progress: 0, queued: 0 (waited: 413s)\n",
      "In progress: 0, queued: 0 (waited: 413s)\n",
      "In progress: 0, queued: 0 (waited: 413s)\n",
      "In progress: 0, queued: 0 (waited: 413s)\n",
      "In progress: 0, queued: 0 (waited: 474s)\n",
      "In progress: 0, queued: 0 (waited: 474s)\n",
      "In progress: 0, queued: 0 (waited: 474s)\n",
      "Training of project 'Parallel Project - 2' finished in 0:08:38.625724\n",
      "Training of project 'Parallel Project - 3' finished in 0:08:38.687918\n",
      "Training of project 'Parallel Project - 4' finished in 0:08:38.785352\n",
      "In progress: 0, queued: 0 (waited: 474s)\n",
      "In progress: 0, queued: 0 (waited: 474s)\n",
      "Training of project 'Parallel Project - 1' finished in 0:08:43.753684\n",
      "Training of project 'Parallel Project - 5' finished in 0:08:43.793173\n"
     ]
    }
   ],
   "source": [
    "with f.ThreadPoolExecutor(max_workers=THREAD_POOL_WORKERS) as executor:\n",
    "    allFutures = {executor.submit(thread_function, pr, datetime.datetime.now()): pr for pr in project_list}\n",
    "\n",
    "    for future in f.as_completed(allFutures):\n",
    "        pr = allFutures[future]\n",
    "        try:\n",
    "            elapsed_time = future.result()\n",
    "        except Exception as exc:\n",
    "            print(f\"Training of project '{pr.project_name}' generated an exception: {exc}\")\n",
    "        else:\n",
    "            print(f\"Training of project '{pr.project_name}' finished in {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1ffa2d",
   "metadata": {},
   "source": [
    "The training time for a multithreaded approach will depend on multiple factors (CPU/RAM load, network bandwidth, etc.) and will vary for the different runs. The average training time is **8min 40s**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b93938",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae66c03",
   "metadata": {},
   "source": [
    "Three experiments are performed in this AI Accelerator:\n",
    "\n",
    "- Training one project (training time: **292s**)\n",
    "- Training one project with advanced options (training time: **170s**)\n",
    "- Training five projects in parallel (training time: **520s**)\n",
    "\n",
    "Training five projects sequentially would take **1460s**, while training five projects in parallel took **520s** (**64%** gain i.e. **2.8 times** faster). Combining parallel training with advanced project options can also decrease overall training time.\n",
    "\n",
    "Taking into account the above mentioned numbers, you can conclude that building model factories using multithreaded approach can be really helpful during the ML experimentation phase especially if there is a need to train models for the use cases with thousands of SKUs. The main advantage of the presented approach is an absence of the third party libraries, the full process is based on the Python *threading* library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
